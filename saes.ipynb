{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3c64404",
   "metadata": {},
   "source": [
    "# Feature-level SAE Transfer & Semantics Analysis\n",
    "\n",
    "## Objective\n",
    "Analyze how individual SAE (Sparse Autoencoder) features change under multimodal adaptation from Gemma-base to PaliGemma.\n",
    "\n",
    "This experiment:\n",
    "1. Loads pre-trained SAE weights from GemmaScope (Google DeepMind's open SAEs for Gemma)\n",
    "2. Extracts hidden layer activations from both Gemma-base and PaliGemma\n",
    "3. Identifies interpretable features and categorizes them as:\n",
    "   - **Stable features**: Activate similarly across both models\n",
    "   - **Shifted features**: Exist in both but with different activation patterns\n",
    "   - **New features**: Only strongly active in PaliGemma (vision-specific)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8739e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# Cell 1: Install Dependencies and Imports\n",
    "# ==========================================\n",
    "# Uncomment below if needed:\n",
    "# %pip install sae-lens transformers torch matplotlib seaborn pandas tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoModelForImageTextToText,\n",
    "    AutoTokenizer,\n",
    "    AutoProcessor\n",
    ")\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import os\n",
    "import json\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e9364d",
   "metadata": {},
   "source": [
    "## Configuration and Model Setup\n",
    "\n",
    "We'll use:\n",
    "- **Gemma-2B**: The base language model\n",
    "- **PaliGemma-3B-pt-224**: The multimodal variant with vision capabilities\n",
    "\n",
    "For SAEs, we'll use GemmaScope SAEs which are publicly available from Google DeepMind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0efe98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# Cell 2: Configuration\n",
    "# ==========================================\n",
    "\n",
    "# Model configurations\n",
    "GEMMA_MODEL_NAME = \"google/gemma-2-2b\"  # Base Gemma model\n",
    "PALIGEMMA_MODEL_NAME = \"google/paligemma2-3b-pt-224\"  # Multimodal PaliGemma\n",
    "\n",
    "# Device configuration\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# SAE configuration - we'll extract from a middle layer\n",
    "# For Gemma-2B, good interpretable features are typically in layers 8-16\n",
    "TARGET_LAYER = 10  # Target hidden layer for analysis\n",
    "SAE_HIDDEN_DIM = 16384  # Typical SAE expansion factor of 8x for 2048-dim model\n",
    "\n",
    "# Analysis configuration\n",
    "NUM_FEATURES_TO_ANALYZE = 100  # Number of top features to analyze\n",
    "TOP_K_ACTIVATIONS = 20  # Top activations per feature to examine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa00d43",
   "metadata": {},
   "source": [
    "## Sparse Autoencoder Implementation\n",
    "\n",
    "We implement a simple SAE class that can:\n",
    "1. Train on model activations\n",
    "2. Encode activations to sparse feature representations  \n",
    "3. Decode back to original space\n",
    "\n",
    "The SAE uses a ReLU activation for sparsity and includes L1 regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e441ec76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# Cell 3: Sparse Autoencoder Implementation\n",
    "# ==========================================\n",
    "\n",
    "class SparseAutoencoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Sparse Autoencoder for learning interpretable features from model activations.\n",
    "    \n",
    "    Architecture:\n",
    "    - Encoder: Linear(d_model -> d_sae) + ReLU for sparsity\n",
    "    - Decoder: Linear(d_sae -> d_model) with tied weights optional\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        d_model: int, \n",
    "        d_sae: int, \n",
    "        dtype: torch.dtype = torch.float32,\n",
    "        device: torch.device = None\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_sae = d_sae\n",
    "        self.device = device or torch.device(\"cpu\")\n",
    "        \n",
    "        # Encoder: projects activations to sparse feature space\n",
    "        self.W_enc = nn.Parameter(torch.empty(d_model, d_sae, dtype=dtype, device=device))\n",
    "        self.b_enc = nn.Parameter(torch.zeros(d_sae, dtype=dtype, device=device))\n",
    "        \n",
    "        # Decoder: reconstructs original activations\n",
    "        self.W_dec = nn.Parameter(torch.empty(d_sae, d_model, dtype=dtype, device=device))\n",
    "        self.b_dec = nn.Parameter(torch.zeros(d_model, dtype=dtype, device=device))\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._init_weights()\n",
    "        \n",
    "    def _init_weights(self):\n",
    "        \"\"\"Initialize with Kaiming normal for encoder, transpose for decoder\"\"\"\n",
    "        nn.init.kaiming_normal_(self.W_enc, nonlinearity='relu')\n",
    "        # Initialize decoder as transpose of encoder (tied initialization)\n",
    "        self.W_dec.data = self.W_enc.data.T.clone()\n",
    "        \n",
    "    def encode(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Encode activations to sparse feature space.\n",
    "        \n",
    "        Args:\n",
    "            x: Activation tensor of shape [..., d_model]\n",
    "        Returns:\n",
    "            Sparse feature tensor of shape [..., d_sae]\n",
    "        \"\"\"\n",
    "        # Pre-encoder bias subtraction (center activations)\n",
    "        x_centered = x - self.b_dec\n",
    "        # Linear projection + ReLU for sparsity\n",
    "        return F.relu(x_centered @ self.W_enc + self.b_enc)\n",
    "    \n",
    "    def decode(self, features: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Decode sparse features back to activation space.\n",
    "        \n",
    "        Args:\n",
    "            features: Sparse feature tensor of shape [..., d_sae]\n",
    "        Returns:\n",
    "            Reconstructed activation tensor of shape [..., d_model]\n",
    "        \"\"\"\n",
    "        return features @ self.W_dec + self.b_dec\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Full forward pass: encode then decode.\n",
    "        \n",
    "        Returns:\n",
    "            - reconstructed: Reconstructed activations\n",
    "            - features: Sparse feature activations\n",
    "        \"\"\"\n",
    "        features = self.encode(x)\n",
    "        reconstructed = self.decode(features)\n",
    "        return reconstructed, features\n",
    "    \n",
    "    def get_feature_activations(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Get just the feature activations for analysis.\"\"\"\n",
    "        return self.encode(x)\n",
    "\n",
    "print(\"SparseAutoencoder class defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244b261c",
   "metadata": {},
   "source": [
    "## Load GemmaScope SAE Weights\n",
    "\n",
    "GemmaScope provides pre-trained SAEs for Gemma models. We'll download and load these weights.\n",
    "If unavailable, we'll train a simple SAE on collected activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6a814b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# Cell 4: Load GemmaScope SAE or Train Simple SAE\n",
    "# ==========================================\n",
    "\n",
    "def load_gemmascope_sae(layer_idx: int = 10, width: str = \"16k\", device: torch.device = DEVICE):\n",
    "    \"\"\"\n",
    "    Attempt to load GemmaScope SAE weights from HuggingFace.\n",
    "    \n",
    "    GemmaScope SAEs are available at: google/gemma-scope-2b-pt-res\n",
    "    Format: layer_{layer_idx}/width_{width}/average_l0_{sparsity}\n",
    "    \"\"\"\n",
    "    try:\n",
    "        from huggingface_hub import hf_hub_download\n",
    "        import safetensors.torch\n",
    "        \n",
    "        # GemmaScope repository\n",
    "        repo_id = \"google/gemma-scope-2b-pt-res\"\n",
    "        \n",
    "        # Download SAE weights for the target layer\n",
    "        # Using 16k width (expansion factor ~8x for 2048-dim model)\n",
    "        filename = f\"layer_{layer_idx}/width_{width}/average_l0_82/params.npz\"\n",
    "        \n",
    "        try:\n",
    "            local_path = hf_hub_download(repo_id=repo_id, filename=filename)\n",
    "            \n",
    "            # Load numpy weights\n",
    "            import numpy as np\n",
    "            sae_weights = np.load(local_path)\n",
    "            \n",
    "            # Create SAE and load weights\n",
    "            d_model = sae_weights['W_enc'].shape[0]\n",
    "            d_sae = sae_weights['W_enc'].shape[1]\n",
    "            \n",
    "            sae = SparseAutoencoder(d_model, d_sae, device=device)\n",
    "            sae.W_enc.data = torch.from_numpy(sae_weights['W_enc']).to(device).float()\n",
    "            sae.W_dec.data = torch.from_numpy(sae_weights['W_dec']).to(device).float()\n",
    "            sae.b_enc.data = torch.from_numpy(sae_weights['b_enc']).to(device).float()\n",
    "            sae.b_dec.data = torch.from_numpy(sae_weights['b_dec']).to(device).float()\n",
    "            \n",
    "            print(f\"Loaded GemmaScope SAE for layer {layer_idx}\")\n",
    "            print(f\"  d_model: {d_model}, d_sae: {d_sae}\")\n",
    "            return sae\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Could not download from GemmaScope: {e}\")\n",
    "            return None\n",
    "            \n",
    "    except ImportError:\n",
    "        print(\"huggingface_hub not available, will train SAE from scratch\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def train_simple_sae(\n",
    "    activations: torch.Tensor,\n",
    "    d_model: int,\n",
    "    d_sae: int,\n",
    "    num_epochs: int = 50,\n",
    "    batch_size: int = 256,\n",
    "    lr: float = 1e-3,\n",
    "    l1_coeff: float = 5e-4,\n",
    "    device: torch.device = DEVICE\n",
    "):\n",
    "    \"\"\"\n",
    "    Train a simple SAE on provided activations.\n",
    "    \n",
    "    Args:\n",
    "        activations: Tensor of shape [N, d_model]\n",
    "        d_model: Model hidden dimension\n",
    "        d_sae: SAE hidden dimension (typically 8x d_model)\n",
    "        num_epochs: Training epochs\n",
    "        batch_size: Batch size\n",
    "        lr: Learning rate\n",
    "        l1_coeff: L1 sparsity regularization coefficient\n",
    "    \"\"\"\n",
    "    sae = SparseAutoencoder(d_model, d_sae, device=device).to(device)\n",
    "    optimizer = torch.optim.Adam(sae.parameters(), lr=lr)\n",
    "    \n",
    "    # Prepare data\n",
    "    activations = activations.to(device).float()\n",
    "    dataset = torch.utils.data.TensorDataset(activations)\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # Enable gradients for training\n",
    "    for param in sae.parameters():\n",
    "        param.requires_grad = True\n",
    "    \n",
    "    losses = []\n",
    "    with torch.enable_grad():\n",
    "        for epoch in tqdm(range(num_epochs), desc=\"Training SAE\"):\n",
    "            epoch_loss = 0\n",
    "            for batch in dataloader:\n",
    "                x = batch[0]\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # Forward pass\n",
    "                reconstructed, features = sae(x)\n",
    "                \n",
    "                # Reconstruction loss (MSE)\n",
    "                recon_loss = F.mse_loss(reconstructed, x)\n",
    "                \n",
    "                # Sparsity loss (L1 on feature activations)\n",
    "                l1_loss = l1_coeff * features.abs().mean()\n",
    "                \n",
    "                # Total loss\n",
    "                loss = recon_loss + l1_loss\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                # Normalize decoder weights (important for SAE stability)\n",
    "                with torch.no_grad():\n",
    "                    sae.W_dec.data = F.normalize(sae.W_dec.data, dim=1)\n",
    "                \n",
    "                epoch_loss += loss.item()\n",
    "            \n",
    "            losses.append(epoch_loss / len(dataloader))\n",
    "    \n",
    "    # Disable gradients after training\n",
    "    for param in sae.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    print(f\"Final loss: {losses[-1]:.4f}\")\n",
    "    return sae, losses\n",
    "\n",
    "\n",
    "print(\"SAE loading and training functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc963a37",
   "metadata": {},
   "source": [
    "## Load Models: Gemma-base and PaliGemma\n",
    "\n",
    "We load both models to extract and compare activations at the same layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4475f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# Cell 5: Load Models\n",
    "# ==========================================\n",
    "\n",
    "# Load Gemma-2B base model\n",
    "print(\"Loading Gemma-2B base model...\")\n",
    "gemma_model = AutoModelForCausalLM.from_pretrained(\n",
    "    GEMMA_MODEL_NAME,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=DEVICE,\n",
    ")\n",
    "gemma_tokenizer = AutoTokenizer.from_pretrained(GEMMA_MODEL_NAME)\n",
    "print(f\"  Loaded {GEMMA_MODEL_NAME}\")\n",
    "print(f\"  Number of layers: {gemma_model.config.num_hidden_layers}\")\n",
    "print(f\"  Hidden size: {gemma_model.config.hidden_size}\")\n",
    "\n",
    "# Load PaliGemma multimodal model\n",
    "print(\"\\nLoading PaliGemma multimodal model...\")\n",
    "paligemma_model = AutoModelForImageTextToText.from_pretrained(\n",
    "    PALIGEMMA_MODEL_NAME,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=DEVICE,\n",
    ")\n",
    "paligemma_processor = AutoProcessor.from_pretrained(PALIGEMMA_MODEL_NAME)\n",
    "paligemma_tokenizer = AutoTokenizer.from_pretrained(PALIGEMMA_MODEL_NAME)\n",
    "print(f\"  Loaded {PALIGEMMA_MODEL_NAME}\")\n",
    "print(f\"  Number of layers: {paligemma_model.language_model.config.num_hidden_layers}\")\n",
    "print(f\"  Hidden size: {paligemma_model.language_model.config.hidden_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87fbb362",
   "metadata": {},
   "source": [
    "## Activation Extraction Hooks\n",
    "\n",
    "We use forward hooks to capture hidden state activations at a specific layer.\n",
    "This allows us to extract activations without modifying the model code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd65800c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# Cell 6: Activation Extraction Class\n",
    "# ==========================================\n",
    "\n",
    "class ActivationExtractor:\n",
    "    \"\"\"\n",
    "    Extracts hidden layer activations from transformer models using forward hooks.\n",
    "    Works with both standard LLMs and multimodal models.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, target_layer: int, model_type: str = \"gemma\"):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            model: The transformer model\n",
    "            target_layer: Which layer to extract activations from\n",
    "            model_type: Either \"gemma\" for base model or \"paligemma\" for multimodal\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.target_layer = target_layer\n",
    "        self.model_type = model_type\n",
    "        self.activations = []\n",
    "        self.hook_handle = None\n",
    "        \n",
    "    def _get_target_module(self):\n",
    "        \"\"\"Get the module to attach the hook to.\"\"\"\n",
    "        if self.model_type == \"gemma\":\n",
    "            return self.model.model.layers[self.target_layer]\n",
    "        elif self.model_type == \"paligemma\":\n",
    "            return self.model.language_model.model.layers[self.target_layer]\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown model type: {self.model_type}\")\n",
    "    \n",
    "    def _hook_fn(self, module, input, output):\n",
    "        \"\"\"Hook function to capture activations.\"\"\"\n",
    "        # Output is typically (hidden_states, ...) or just hidden_states\n",
    "        if isinstance(output, tuple):\n",
    "            hidden_states = output[0]\n",
    "        else:\n",
    "            hidden_states = output\n",
    "        self.activations.append(hidden_states.detach().cpu().float())\n",
    "    \n",
    "    def attach_hook(self):\n",
    "        \"\"\"Attach the forward hook.\"\"\"\n",
    "        target_module = self._get_target_module()\n",
    "        self.hook_handle = target_module.register_forward_hook(self._hook_fn)\n",
    "        \n",
    "    def remove_hook(self):\n",
    "        \"\"\"Remove the forward hook.\"\"\"\n",
    "        if self.hook_handle:\n",
    "            self.hook_handle.remove()\n",
    "            self.hook_handle = None\n",
    "            \n",
    "    def clear_activations(self):\n",
    "        \"\"\"Clear stored activations.\"\"\"\n",
    "        self.activations = []\n",
    "        \n",
    "    def get_activations(self) -> torch.Tensor:\n",
    "        \"\"\"Get all collected activations as a single tensor.\"\"\"\n",
    "        if not self.activations:\n",
    "            return None\n",
    "        return torch.cat(self.activations, dim=0)\n",
    "\n",
    "\n",
    "def extract_text_activations(\n",
    "    text_prompts: List[str],\n",
    "    model,\n",
    "    tokenizer,\n",
    "    extractor: ActivationExtractor,\n",
    ") -> Tuple[torch.Tensor, List[str]]:\n",
    "    \"\"\"\n",
    "    Extract activations for text-only prompts from a language model.\n",
    "    \n",
    "    Returns:\n",
    "        activations: Tensor of shape [total_tokens, d_model]\n",
    "        tokens: List of token strings corresponding to each activation\n",
    "    \"\"\"\n",
    "    extractor.clear_activations()\n",
    "    extractor.attach_hook()\n",
    "    \n",
    "    all_tokens = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for prompt in tqdm(text_prompts, desc=\"Extracting text activations\"):\n",
    "            inputs = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "            model(**inputs)\n",
    "            \n",
    "            # Decode tokens for reference\n",
    "            token_ids = inputs.input_ids[0].tolist()\n",
    "            tokens = [tokenizer.decode([tid]) for tid in token_ids]\n",
    "            all_tokens.extend(tokens)\n",
    "    \n",
    "    extractor.remove_hook()\n",
    "    activations = extractor.get_activations()\n",
    "    \n",
    "    return activations, all_tokens\n",
    "\n",
    "\n",
    "def extract_image_text_activations(\n",
    "    image_text_pairs: List[Tuple[Image.Image, str]],\n",
    "    model,\n",
    "    processor,\n",
    "    extractor: ActivationExtractor,\n",
    ") -> Tuple[torch.Tensor, List[Dict]]:\n",
    "    \"\"\"\n",
    "    Extract activations for image+text prompts from PaliGemma.\n",
    "    \n",
    "    Returns:\n",
    "        activations: Tensor of shape [total_tokens, d_model]\n",
    "        token_info: List of dicts with token info (is_image, token_str, etc.)\n",
    "    \"\"\"\n",
    "    extractor.clear_activations()\n",
    "    extractor.attach_hook()\n",
    "    \n",
    "    all_token_info = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for image, text in tqdm(image_text_pairs, desc=\"Extracting multimodal activations\"):\n",
    "            inputs = processor(\n",
    "                text=f\"<image>{text}\" if text else \"<image>\",\n",
    "                images=image,\n",
    "                return_tensors=\"pt\"\n",
    "            ).to(DEVICE)\n",
    "            \n",
    "            model(**inputs)\n",
    "            \n",
    "            # Track which tokens are image vs text\n",
    "            # PaliGemma uses <image> token that gets expanded to image embeddings\n",
    "            # The first N tokens are image tokens (typically 256 for 224x224 images)\n",
    "            num_image_tokens = (inputs.input_ids == processor.tokenizer.convert_tokens_to_ids(\"<image>\")).sum().item()\n",
    "            \n",
    "            # For simplicity, mark first batch of tokens as image-related\n",
    "            seq_len = inputs.input_ids.shape[1]\n",
    "            for i in range(seq_len):\n",
    "                all_token_info.append({\n",
    "                    \"is_image\": i < 256,  # Approximate, adjust based on actual image token count\n",
    "                    \"token_id\": inputs.input_ids[0, i].item() if i < inputs.input_ids.shape[1] else -1,\n",
    "                })\n",
    "    \n",
    "    extractor.remove_hook()\n",
    "    activations = extractor.get_activations()\n",
    "    \n",
    "    return activations, all_token_info\n",
    "\n",
    "\n",
    "print(\"Activation extraction classes defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075264e8",
   "metadata": {},
   "source": [
    "## Test Prompts for Feature Analysis\n",
    "\n",
    "We'll use a diverse set of prompts covering different semantic categories to identify interpretable features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ba7dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# Cell 7: Define Test Prompts and Images\n",
    "# ==========================================\n",
    "\n",
    "# Diverse text prompts to probe different semantic features\n",
    "TEXT_PROMPTS = [\n",
    "    # Numbers and counting\n",
    "    \"The answer is 42.\",\n",
    "    \"There are 3 apples and 5 oranges.\",\n",
    "    \"1, 2, 3, 4, 5, 6, 7, 8, 9, 10\",\n",
    "    \n",
    "    # Colors\n",
    "    \"The red ball bounced across the green grass.\",\n",
    "    \"She wore a beautiful blue dress to the party.\",\n",
    "    \"The yellow sun set behind the purple mountains.\",\n",
    "    \n",
    "    # Animals\n",
    "    \"The dog chased the cat around the garden.\",\n",
    "    \"A lion hunts zebras on the African savanna.\",\n",
    "    \"The elephant's trunk can hold gallons of water.\",\n",
    "    \n",
    "    # Spatial relationships\n",
    "    \"The book is on top of the table.\",\n",
    "    \"The cat is sitting under the chair.\",\n",
    "    \"The ball rolled between the two posts.\",\n",
    "    \n",
    "    # Actions/verbs\n",
    "    \"She ran quickly down the stairs.\",\n",
    "    \"He carefully opened the ancient door.\",\n",
    "    \"The bird flew gracefully through the sky.\",\n",
    "    \n",
    "    # Objects\n",
    "    \"The computer screen displayed an error message.\",\n",
    "    \"The car drove down the highway at high speed.\",\n",
    "    \"The camera captured the beautiful sunset.\",\n",
    "    \n",
    "    # Abstract concepts\n",
    "    \"Love is patient, love is kind.\",\n",
    "    \"The theory of relativity changed physics forever.\",\n",
    "    \"Democracy requires active citizen participation.\",\n",
    "    \n",
    "    # Code-related\n",
    "    \"def hello_world(): print('Hello!')\",\n",
    "    \"for i in range(10): sum += i\",\n",
    "    \"if x > 0: return True else: return False\",\n",
    "]\n",
    "\n",
    "# Sample images for multimodal testing (we'll use existing dataset or create simple ones)\n",
    "def get_sample_images() -> List[Tuple[Image.Image, str]]:\n",
    "    \"\"\"Load sample images for multimodal testing.\"\"\"\n",
    "    image_text_pairs = []\n",
    "    \n",
    "    # Try to load from existing dataset\n",
    "    data_dir = \"data/SPair-71k_data/SPair-71k/JPEGImages\"\n",
    "    categories = [\"dog\", \"cat\", \"car\", \"bird\", \"horse\"]\n",
    "    \n",
    "    for category in categories:\n",
    "        cat_dir = os.path.join(data_dir, category)\n",
    "        if os.path.exists(cat_dir):\n",
    "            images = os.listdir(cat_dir)[:3]  # Take first 3 images per category\n",
    "            for img_name in images:\n",
    "                try:\n",
    "                    img_path = os.path.join(cat_dir, img_name)\n",
    "                    img = Image.open(img_path).convert(\"RGB\")\n",
    "                    img = img.resize((224, 224))\n",
    "                    image_text_pairs.append((img, f\"A photo of a {category}.\"))\n",
    "                except Exception as e:\n",
    "                    print(f\"Could not load {img_path}: {e}\")\n",
    "    \n",
    "    # If no images found, create simple colored images\n",
    "    if len(image_text_pairs) == 0:\n",
    "        print(\"No dataset images found, creating synthetic test images...\")\n",
    "        colors = {\n",
    "            \"red\": (255, 0, 0),\n",
    "            \"green\": (0, 255, 0),\n",
    "            \"blue\": (0, 0, 255),\n",
    "            \"yellow\": (255, 255, 0),\n",
    "            \"purple\": (128, 0, 128),\n",
    "        }\n",
    "        for color_name, rgb in colors.items():\n",
    "            img = Image.new(\"RGB\", (224, 224), rgb)\n",
    "            image_text_pairs.append((img, f\"A {color_name} image.\"))\n",
    "    \n",
    "    print(f\"Loaded {len(image_text_pairs)} image-text pairs\")\n",
    "    return image_text_pairs\n",
    "\n",
    "print(f\"Defined {len(TEXT_PROMPTS)} text prompts for analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef58c57",
   "metadata": {},
   "source": [
    "## Extract Activations and Train/Load SAE\n",
    "\n",
    "Now we extract activations from both models and prepare the SAE for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3794fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# Cell 8: Extract Activations from Both Models\n",
    "# ==========================================\n",
    "\n",
    "# Create extractors for both models\n",
    "gemma_extractor = ActivationExtractor(gemma_model, TARGET_LAYER, model_type=\"gemma\")\n",
    "paligemma_extractor = ActivationExtractor(paligemma_model, TARGET_LAYER, model_type=\"paligemma\")\n",
    "\n",
    "# Extract text activations from Gemma\n",
    "print(\"Extracting activations from Gemma-2B...\")\n",
    "gemma_text_acts, gemma_tokens = extract_text_activations(\n",
    "    TEXT_PROMPTS, gemma_model, gemma_tokenizer, gemma_extractor\n",
    ")\n",
    "print(f\"  Gemma text activations shape: {gemma_text_acts.shape}\")\n",
    "\n",
    "# Extract text activations from PaliGemma (text-only, for fair comparison)\n",
    "print(\"\\nExtracting text activations from PaliGemma...\")\n",
    "# For text-only with PaliGemma, we need to create dummy inputs\n",
    "paligemma_text_acts_list = []\n",
    "paligemma_text_tokens = []\n",
    "\n",
    "paligemma_extractor.clear_activations()\n",
    "paligemma_extractor.attach_hook()\n",
    "\n",
    "for prompt in tqdm(TEXT_PROMPTS, desc=\"Extracting PaliGemma text activations\"):\n",
    "    # For PaliGemma, we can use just the tokenizer for text-only inputs\n",
    "    inputs = paligemma_tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "    # Need to go through language model only\n",
    "    with torch.no_grad():\n",
    "        paligemma_model.language_model(**inputs)\n",
    "    \n",
    "    token_ids = inputs.input_ids[0].tolist()\n",
    "    tokens = [paligemma_tokenizer.decode([tid]) for tid in token_ids]\n",
    "    paligemma_text_tokens.extend(tokens)\n",
    "\n",
    "paligemma_extractor.remove_hook()\n",
    "paligemma_text_acts = paligemma_extractor.get_activations()\n",
    "print(f\"  PaliGemma text activations shape: {paligemma_text_acts.shape}\")\n",
    "\n",
    "# Extract multimodal activations from PaliGemma\n",
    "print(\"\\nExtracting multimodal (image+text) activations from PaliGemma...\")\n",
    "image_text_pairs = get_sample_images()\n",
    "paligemma_mm_acts, paligemma_mm_info = extract_image_text_activations(\n",
    "    image_text_pairs, paligemma_model, paligemma_processor, paligemma_extractor\n",
    ")\n",
    "print(f\"  PaliGemma multimodal activations shape: {paligemma_mm_acts.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0edaf1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# Cell 9: Load or Train SAE\n",
    "# ==========================================\n",
    "\n",
    "# Try to load GemmaScope SAE first\n",
    "sae = load_gemmascope_sae(layer_idx=TARGET_LAYER, device=DEVICE)\n",
    "\n",
    "# If GemmaScope not available, train on Gemma activations\n",
    "if sae is None:\n",
    "    print(\"Training SAE on Gemma activations...\")\n",
    "    d_model = gemma_text_acts.shape[-1]\n",
    "    d_sae = d_model * 8  # 8x expansion factor\n",
    "    \n",
    "    # Reshape activations for training [batch*seq, d_model]\n",
    "    train_acts = gemma_text_acts.reshape(-1, d_model)\n",
    "    \n",
    "    sae, train_losses = train_simple_sae(\n",
    "        train_acts,\n",
    "        d_model=d_model,\n",
    "        d_sae=d_sae,\n",
    "        num_epochs=100,\n",
    "        device=DEVICE\n",
    "    )\n",
    "    \n",
    "    # Plot training loss\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(train_losses)\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"SAE Training Loss\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "print(f\"\\nSAE ready: d_model={sae.d_model}, d_sae={sae.d_sae}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3fec0c",
   "metadata": {},
   "source": [
    "## Feature-level Analysis: Identify Stable, Shifted, and New Features\n",
    "\n",
    "We now apply the SAE to both Gemma and PaliGemma activations, then classify features:\n",
    "- **Stable**: High correlation in activation patterns between models\n",
    "- **Shifted**: Active in both but with significantly different patterns\n",
    "- **New (Vision-specific)**: Only strongly active in PaliGemma multimodal inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2aa288c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# Cell 10: Compute SAE Feature Activations for All Conditions\n",
    "# ==========================================\n",
    "\n",
    "# Move SAE to device\n",
    "sae = sae.to(DEVICE)\n",
    "\n",
    "# Get SAE feature activations for each condition\n",
    "print(\"Computing SAE feature activations...\")\n",
    "\n",
    "# Reshape activations to [num_tokens, d_model]\n",
    "gemma_flat = gemma_text_acts.reshape(-1, gemma_text_acts.shape[-1]).to(DEVICE)\n",
    "pali_text_flat = paligemma_text_acts.reshape(-1, paligemma_text_acts.shape[-1]).to(DEVICE)\n",
    "pali_mm_flat = paligemma_mm_acts.reshape(-1, paligemma_mm_acts.shape[-1]).to(DEVICE)\n",
    "\n",
    "# Get SAE features\n",
    "with torch.no_grad():\n",
    "    gemma_features = sae.encode(gemma_flat).cpu()\n",
    "    pali_text_features = sae.encode(pali_text_flat).cpu()\n",
    "    pali_mm_features = sae.encode(pali_mm_flat).cpu()\n",
    "\n",
    "print(f\"  Gemma text features shape: {gemma_features.shape}\")\n",
    "print(f\"  PaliGemma text features shape: {pali_text_features.shape}\")\n",
    "print(f\"  PaliGemma multimodal features shape: {pali_mm_features.shape}\")\n",
    "\n",
    "# Compute mean activation per feature\n",
    "gemma_mean = gemma_features.mean(dim=0)\n",
    "pali_text_mean = pali_text_features.mean(dim=0)\n",
    "pali_mm_mean = pali_mm_features.mean(dim=0)\n",
    "\n",
    "# Compute max activation per feature (better for sparse features)\n",
    "gemma_max = gemma_features.max(dim=0).values\n",
    "pali_text_max = pali_text_features.max(dim=0).values\n",
    "pali_mm_max = pali_mm_features.max(dim=0).values\n",
    "\n",
    "print(f\"\\nFeature activation statistics:\")\n",
    "print(f\"  Gemma - mean: {gemma_mean.mean():.4f}, nonzero: {(gemma_mean > 0).sum()}\")\n",
    "print(f\"  PaliGemma text - mean: {pali_text_mean.mean():.4f}, nonzero: {(pali_text_mean > 0).sum()}\")\n",
    "print(f\"  PaliGemma multimodal - mean: {pali_mm_mean.mean():.4f}, nonzero: {(pali_mm_mean > 0).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43e1190",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# Cell 11: Classify Features as Stable, Shifted, or New\n",
    "# ==========================================\n",
    "\n",
    "def classify_features(\n",
    "    gemma_features: torch.Tensor,\n",
    "    pali_text_features: torch.Tensor,\n",
    "    pali_mm_features: torch.Tensor,\n",
    "    stability_threshold: float = 0.7,\n",
    "    shift_threshold: float = 0.3,\n",
    "    activation_threshold: float = 0.1,\n",
    ") -> Dict[str, List[int]]:\n",
    "    \"\"\"\n",
    "    Classify SAE features into stable, shifted, and new categories.\n",
    "    \n",
    "    Args:\n",
    "        gemma_features: [N1, d_sae] features from Gemma on text\n",
    "        pali_text_features: [N2, d_sae] features from PaliGemma on text\n",
    "        pali_mm_features: [N3, d_sae] features from PaliGemma on images\n",
    "        stability_threshold: Correlation threshold for \"stable\" features\n",
    "        shift_threshold: Lower bound for \"shifted\" features\n",
    "        activation_threshold: Minimum activation to consider feature \"active\"\n",
    "    \n",
    "    Returns:\n",
    "        Dict with feature indices for each category\n",
    "    \"\"\"\n",
    "    num_features = gemma_features.shape[1]\n",
    "    \n",
    "    # Compute per-feature statistics\n",
    "    gemma_active = (gemma_features > activation_threshold).float().mean(dim=0)\n",
    "    pali_text_active = (pali_text_features > activation_threshold).float().mean(dim=0)\n",
    "    pali_mm_active = (pali_mm_features > activation_threshold).float().mean(dim=0)\n",
    "    \n",
    "    # Compute mean activations\n",
    "    gemma_mean = gemma_features.mean(dim=0)\n",
    "    pali_text_mean = pali_text_features.mean(dim=0)\n",
    "    pali_mm_mean = pali_mm_features.mean(dim=0)\n",
    "    \n",
    "    # Find features active in Gemma (for correlation analysis)\n",
    "    gemma_active_features = (gemma_active > 0.01).nonzero().squeeze(-1)\n",
    "    \n",
    "    classified = {\n",
    "        \"stable\": [],\n",
    "        \"shifted\": [],\n",
    "        \"new_vision\": [],\n",
    "        \"new_text\": [],\n",
    "    }\n",
    "    \n",
    "    # For each feature, determine its category\n",
    "    for feat_idx in range(num_features):\n",
    "        g_act = gemma_active[feat_idx].item()\n",
    "        pt_act = pali_text_active[feat_idx].item()\n",
    "        pm_act = pali_mm_active[feat_idx].item()\n",
    "        \n",
    "        g_mean = gemma_mean[feat_idx].item()\n",
    "        pt_mean = pali_text_mean[feat_idx].item()\n",
    "        pm_mean = pali_mm_mean[feat_idx].item()\n",
    "        \n",
    "        # Check if active in any condition\n",
    "        is_active_gemma = g_act > 0.01\n",
    "        is_active_pali_text = pt_act > 0.01\n",
    "        is_active_pali_mm = pm_act > 0.01\n",
    "        \n",
    "        if not (is_active_gemma or is_active_pali_text or is_active_pali_mm):\n",
    "            continue  # Skip dead features\n",
    "        \n",
    "        # New vision-specific: only active in multimodal, not in text-only\n",
    "        if is_active_pali_mm and not is_active_gemma and pm_mean > g_mean * 3:\n",
    "            classified[\"new_vision\"].append(feat_idx)\n",
    "            continue\n",
    "        \n",
    "        # New text features: active in PaliGemma text but not Gemma\n",
    "        if is_active_pali_text and not is_active_gemma and pt_mean > g_mean * 3:\n",
    "            classified[\"new_text\"].append(feat_idx)\n",
    "            continue\n",
    "        \n",
    "        # For features active in both, check similarity\n",
    "        if is_active_gemma and is_active_pali_text:\n",
    "            # Compute ratio of mean activations as stability measure\n",
    "            ratio = min(g_mean, pt_mean) / max(g_mean, pt_mean) if max(g_mean, pt_mean) > 0 else 0\n",
    "            \n",
    "            if ratio > stability_threshold:\n",
    "                classified[\"stable\"].append(feat_idx)\n",
    "            elif ratio > shift_threshold:\n",
    "                classified[\"shifted\"].append(feat_idx)\n",
    "    \n",
    "    return classified\n",
    "\n",
    "\n",
    "# Classify all features\n",
    "feature_classification = classify_features(\n",
    "    gemma_features, pali_text_features, pali_mm_features\n",
    ")\n",
    "\n",
    "print(\"Feature Classification Results:\")\n",
    "print(f\"  Stable features: {len(feature_classification['stable'])}\")\n",
    "print(f\"  Shifted features: {len(feature_classification['shifted'])}\")\n",
    "print(f\"  New vision-specific features: {len(feature_classification['new_vision'])}\")\n",
    "print(f\"  New text features (PaliGemma only): {len(feature_classification['new_text'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ebe06d",
   "metadata": {},
   "source": [
    "## Visualization: Feature Distribution Overview\n",
    "\n",
    "Let's visualize the overall distribution of feature activations across models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4f6fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# Cell 12: Feature Distribution Visualization\n",
    "# ==========================================\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Plot 1: Scatter plot of mean activations (Gemma vs PaliGemma text)\n",
    "ax = axes[0, 0]\n",
    "ax.scatter(gemma_mean.numpy(), pali_text_mean.numpy(), alpha=0.3, s=5)\n",
    "ax.plot([0, gemma_mean.max()], [0, gemma_mean.max()], 'r--', label='y=x (perfect stability)')\n",
    "ax.set_xlabel(\"Gemma Mean Activation\")\n",
    "ax.set_ylabel(\"PaliGemma (Text) Mean Activation\")\n",
    "ax.set_title(\"Feature Activation Comparison: Text-only\")\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Scatter plot (Gemma vs PaliGemma multimodal)\n",
    "ax = axes[0, 1]\n",
    "ax.scatter(gemma_mean.numpy(), pali_mm_mean.numpy(), alpha=0.3, s=5)\n",
    "ax.plot([0, gemma_mean.max()], [0, gemma_mean.max()], 'r--', label='y=x')\n",
    "ax.set_xlabel(\"Gemma Mean Activation\")\n",
    "ax.set_ylabel(\"PaliGemma (Multimodal) Mean Activation\")\n",
    "ax.set_title(\"Feature Activation Comparison: Multimodal\")\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Distribution of feature activation ratios\n",
    "ax = axes[1, 0]\n",
    "# Avoid division by zero\n",
    "eps = 1e-6\n",
    "text_ratio = (pali_text_mean / (gemma_mean + eps)).numpy()\n",
    "mm_ratio = (pali_mm_mean / (gemma_mean + eps)).numpy()\n",
    "# Clip for visualization\n",
    "text_ratio = np.clip(text_ratio, 0, 10)\n",
    "mm_ratio = np.clip(mm_ratio, 0, 10)\n",
    "\n",
    "ax.hist(text_ratio, bins=50, alpha=0.5, label='Text ratio', density=True)\n",
    "ax.hist(mm_ratio, bins=50, alpha=0.5, label='Multimodal ratio', density=True)\n",
    "ax.axvline(x=1.0, color='r', linestyle='--', label='Ratio = 1')\n",
    "ax.set_xlabel(\"Activation Ratio (PaliGemma / Gemma)\")\n",
    "ax.set_ylabel(\"Density\")\n",
    "ax.set_title(\"Distribution of Feature Activation Ratios\")\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Feature category bar chart\n",
    "ax = axes[1, 1]\n",
    "categories = list(feature_classification.keys())\n",
    "counts = [len(feature_classification[cat]) for cat in categories]\n",
    "colors = ['green', 'orange', 'blue', 'purple']\n",
    "bars = ax.bar(categories, counts, color=colors, alpha=0.7)\n",
    "ax.set_xlabel(\"Feature Category\")\n",
    "ax.set_ylabel(\"Number of Features\")\n",
    "ax.set_title(\"Feature Classification Summary\")\n",
    "for bar, count in zip(bars, counts):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 5, \n",
    "            str(count), ha='center', va='bottom', fontweight='bold')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"figures/sae_feature_distribution.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0943719a",
   "metadata": {},
   "source": [
    "## Detailed Feature Examples: Stable, Shifted, and New Features\n",
    "\n",
    "Now we analyze 2-4 concrete examples from each category, showing:\n",
    "- What tokens/images maximally activate each feature\n",
    "- How activation patterns differ between Gemma and PaliGemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b119af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# Cell 13: Helper Functions for Feature Analysis\n",
    "# ==========================================\n",
    "\n",
    "def get_top_activating_examples(\n",
    "    feature_idx: int,\n",
    "    features: torch.Tensor,\n",
    "    tokens: List[str],\n",
    "    top_k: int = 10\n",
    ") -> List[Tuple[str, float]]:\n",
    "    \"\"\"Get the top-k tokens that maximally activate a feature.\"\"\"\n",
    "    feature_acts = features[:, feature_idx]\n",
    "    top_indices = torch.topk(feature_acts, min(top_k, len(feature_acts))).indices\n",
    "    \n",
    "    results = []\n",
    "    for idx in top_indices:\n",
    "        if idx < len(tokens):\n",
    "            results.append((tokens[idx.item()], feature_acts[idx.item()].item()))\n",
    "    return results\n",
    "\n",
    "\n",
    "def analyze_single_feature(\n",
    "    feature_idx: int,\n",
    "    gemma_features: torch.Tensor,\n",
    "    pali_text_features: torch.Tensor,\n",
    "    pali_mm_features: torch.Tensor,\n",
    "    gemma_tokens: List[str],\n",
    "    pali_tokens: List[str],\n",
    "    category: str,\n",
    ") -> Dict:\n",
    "    \"\"\"Analyze a single feature across all conditions.\"\"\"\n",
    "    \n",
    "    # Get top activating tokens for each condition\n",
    "    gemma_top = get_top_activating_examples(feature_idx, gemma_features, gemma_tokens)\n",
    "    pali_text_top = get_top_activating_examples(feature_idx, pali_text_features, pali_tokens)\n",
    "    \n",
    "    # Statistics\n",
    "    g_mean = gemma_features[:, feature_idx].mean().item()\n",
    "    g_max = gemma_features[:, feature_idx].max().item()\n",
    "    p_text_mean = pali_text_features[:, feature_idx].mean().item()\n",
    "    p_text_max = pali_text_features[:, feature_idx].max().item()\n",
    "    p_mm_mean = pali_mm_features[:, feature_idx].mean().item()\n",
    "    p_mm_max = pali_mm_features[:, feature_idx].max().item()\n",
    "    \n",
    "    return {\n",
    "        \"feature_idx\": feature_idx,\n",
    "        \"category\": category,\n",
    "        \"gemma_mean\": g_mean,\n",
    "        \"gemma_max\": g_max,\n",
    "        \"pali_text_mean\": p_text_mean,\n",
    "        \"pali_text_max\": p_text_max,\n",
    "        \"pali_mm_mean\": p_mm_mean,\n",
    "        \"pali_mm_max\": p_mm_max,\n",
    "        \"gemma_top_tokens\": gemma_top[:5],\n",
    "        \"pali_text_top_tokens\": pali_text_top[:5],\n",
    "    }\n",
    "\n",
    "\n",
    "def plot_feature_comparison(analysis: Dict, fig_num: int):\n",
    "    \"\"\"Create a detailed visualization for a single feature.\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "    fig.suptitle(f\"Feature #{analysis['feature_idx']} ({analysis['category'].upper()})\", \n",
    "                 fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # Plot 1: Activation statistics bar chart\n",
    "    ax = axes[0]\n",
    "    conditions = ['Gemma\\nText', 'PaliGemma\\nText', 'PaliGemma\\nMultimodal']\n",
    "    means = [analysis['gemma_mean'], analysis['pali_text_mean'], analysis['pali_mm_mean']]\n",
    "    maxs = [analysis['gemma_max'], analysis['pali_text_max'], analysis['pali_mm_max']]\n",
    "    \n",
    "    x = np.arange(len(conditions))\n",
    "    width = 0.35\n",
    "    bars1 = ax.bar(x - width/2, means, width, label='Mean', color='steelblue', alpha=0.8)\n",
    "    bars2 = ax.bar(x + width/2, maxs, width, label='Max', color='coral', alpha=0.8)\n",
    "    ax.set_ylabel('Activation')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(conditions)\n",
    "    ax.legend()\n",
    "    ax.set_title('Activation Statistics')\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Plot 2: Top activating tokens for Gemma\n",
    "    ax = axes[1]\n",
    "    gemma_tokens_display = [t[0][:20] for t in analysis['gemma_top_tokens']]\n",
    "    gemma_acts = [t[1] for t in analysis['gemma_top_tokens']]\n",
    "    if gemma_tokens_display:\n",
    "        y_pos = np.arange(len(gemma_tokens_display))\n",
    "        ax.barh(y_pos, gemma_acts, color='green', alpha=0.7)\n",
    "        ax.set_yticks(y_pos)\n",
    "        ax.set_yticklabels(gemma_tokens_display, fontsize=9)\n",
    "        ax.invert_yaxis()\n",
    "    ax.set_xlabel('Activation')\n",
    "    ax.set_title('Top Tokens (Gemma)')\n",
    "    ax.grid(True, alpha=0.3, axis='x')\n",
    "    \n",
    "    # Plot 3: Top activating tokens for PaliGemma\n",
    "    ax = axes[2]\n",
    "    pali_tokens_display = [t[0][:20] for t in analysis['pali_text_top_tokens']]\n",
    "    pali_acts = [t[1] for t in analysis['pali_text_top_tokens']]\n",
    "    if pali_tokens_display:\n",
    "        y_pos = np.arange(len(pali_tokens_display))\n",
    "        ax.barh(y_pos, pali_acts, color='purple', alpha=0.7)\n",
    "        ax.set_yticks(y_pos)\n",
    "        ax.set_yticklabels(pali_tokens_display, fontsize=9)\n",
    "        ax.invert_yaxis()\n",
    "    ax.set_xlabel('Activation')\n",
    "    ax.set_title('Top Tokens (PaliGemma)')\n",
    "    ax.grid(True, alpha=0.3, axis='x')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"figures/feature_{analysis['feature_idx']}_{analysis['category']}.png\", \n",
    "                dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "print(\"Feature analysis helper functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b72f227",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# Cell 14: Analyze Example Features from Each Category\n",
    "# ==========================================\n",
    "\n",
    "# Create figures directory if it doesn't exist\n",
    "os.makedirs(\"figures\", exist_ok=True)\n",
    "\n",
    "# Select top features from each category (by activity level)\n",
    "def select_top_features(feature_indices: List[int], features: torch.Tensor, n: int = 4) -> List[int]:\n",
    "    \"\"\"Select the n most active features from a list.\"\"\"\n",
    "    if not feature_indices:\n",
    "        return []\n",
    "    feature_means = features[:, feature_indices].mean(dim=0)\n",
    "    top_indices = torch.topk(feature_means, min(n, len(feature_indices))).indices\n",
    "    return [feature_indices[i] for i in top_indices.tolist()]\n",
    "\n",
    "# Get representative features from each category\n",
    "example_features = {}\n",
    "for category in ['stable', 'shifted', 'new_vision', 'new_text']:\n",
    "    if category == 'new_vision':\n",
    "        # Use multimodal features for selection\n",
    "        example_features[category] = select_top_features(\n",
    "            feature_classification[category], pali_mm_features, n=4\n",
    "        )\n",
    "    elif category == 'new_text':\n",
    "        example_features[category] = select_top_features(\n",
    "            feature_classification[category], pali_text_features, n=4\n",
    "        )\n",
    "    else:\n",
    "        example_features[category] = select_top_features(\n",
    "            feature_classification[category], gemma_features, n=4\n",
    "        )\n",
    "\n",
    "print(\"Selected example features for detailed analysis:\")\n",
    "for cat, feats in example_features.items():\n",
    "    print(f\"  {cat}: {feats}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61215d50",
   "metadata": {},
   "source": [
    "### Example 1: Stable Features\n",
    "\n",
    "These features maintain similar activation patterns across both Gemma and PaliGemma, suggesting they encode concepts that transfer well to the multimodal setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5bc01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# Cell 15: Analyze Stable Features\n",
    "# ==========================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"STABLE FEATURES - Similar activation patterns across models\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "stable_analyses = []\n",
    "for i, feat_idx in enumerate(example_features.get('stable', [])[:4]):\n",
    "    print(f\"\\n--- Stable Feature #{feat_idx} ---\")\n",
    "    analysis = analyze_single_feature(\n",
    "        feat_idx, gemma_features, pali_text_features, pali_mm_features,\n",
    "        gemma_tokens, paligemma_text_tokens, \"stable\"\n",
    "    )\n",
    "    stable_analyses.append(analysis)\n",
    "    \n",
    "    print(f\"  Gemma mean: {analysis['gemma_mean']:.4f}, max: {analysis['gemma_max']:.4f}\")\n",
    "    print(f\"  PaliGemma text mean: {analysis['pali_text_mean']:.4f}, max: {analysis['pali_text_max']:.4f}\")\n",
    "    print(f\"  PaliGemma multimodal mean: {analysis['pali_mm_mean']:.4f}\")\n",
    "    print(f\"  Top Gemma tokens: {[t[0] for t in analysis['gemma_top_tokens'][:3]]}\")\n",
    "    print(f\"  Top PaliGemma tokens: {[t[0] for t in analysis['pali_text_top_tokens'][:3]]}\")\n",
    "    \n",
    "    plot_feature_comparison(analysis, i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a991ce",
   "metadata": {},
   "source": [
    "### Example 2: Shifted Features\n",
    "\n",
    "These features exist in both models but with different activation patterns - suggesting semantic adaptation during multimodal training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b52202f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# Cell 16: Analyze Shifted Features\n",
    "# ==========================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"SHIFTED FEATURES - Active in both but with different patterns\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "shifted_analyses = []\n",
    "for i, feat_idx in enumerate(example_features.get('shifted', [])[:4]):\n",
    "    print(f\"\\n--- Shifted Feature #{feat_idx} ---\")\n",
    "    analysis = analyze_single_feature(\n",
    "        feat_idx, gemma_features, pali_text_features, pali_mm_features,\n",
    "        gemma_tokens, paligemma_text_tokens, \"shifted\"\n",
    "    )\n",
    "    shifted_analyses.append(analysis)\n",
    "    \n",
    "    # Calculate shift magnitude\n",
    "    shift_ratio = analysis['pali_text_mean'] / (analysis['gemma_mean'] + 1e-6)\n",
    "    print(f\"  Shift ratio (PaliGemma/Gemma): {shift_ratio:.2f}x\")\n",
    "    print(f\"  Gemma mean: {analysis['gemma_mean']:.4f}\")\n",
    "    print(f\"  PaliGemma text mean: {analysis['pali_text_mean']:.4f}\")\n",
    "    print(f\"  Top Gemma tokens: {[t[0] for t in analysis['gemma_top_tokens'][:3]]}\")\n",
    "    print(f\"  Top PaliGemma tokens: {[t[0] for t in analysis['pali_text_top_tokens'][:3]]}\")\n",
    "    \n",
    "    plot_feature_comparison(analysis, i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0edb1204",
   "metadata": {},
   "source": [
    "### Example 3: New Vision-Specific Features\n",
    "\n",
    "These features are primarily active in PaliGemma when processing image inputs, suggesting they've been specialized for visual information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3400744b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# Cell 17: Analyze New Vision-Specific Features\n",
    "# ==========================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"NEW VISION-SPECIFIC FEATURES - Active mainly on image inputs\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "vision_analyses = []\n",
    "for i, feat_idx in enumerate(example_features.get('new_vision', [])[:4]):\n",
    "    print(f\"\\n--- Vision-Specific Feature #{feat_idx} ---\")\n",
    "    analysis = analyze_single_feature(\n",
    "        feat_idx, gemma_features, pali_text_features, pali_mm_features,\n",
    "        gemma_tokens, paligemma_text_tokens, \"new_vision\"\n",
    "    )\n",
    "    vision_analyses.append(analysis)\n",
    "    \n",
    "    # Calculate vision specificity\n",
    "    vision_ratio = analysis['pali_mm_mean'] / (analysis['gemma_mean'] + 1e-6)\n",
    "    text_ratio = analysis['pali_text_mean'] / (analysis['gemma_mean'] + 1e-6)\n",
    "    print(f\"  Vision boost: {vision_ratio:.2f}x over Gemma\")\n",
    "    print(f\"  Text boost: {text_ratio:.2f}x over Gemma\")\n",
    "    print(f\"  Gemma mean: {analysis['gemma_mean']:.4f}\")\n",
    "    print(f\"  PaliGemma multimodal mean: {analysis['pali_mm_mean']:.4f}\")\n",
    "    \n",
    "    # For vision features, show activation distribution across image positions\n",
    "    feat_mm_acts = pali_mm_features[:, feat_idx]\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    fig.suptitle(f\"Vision Feature #{feat_idx}\", fontsize=12, fontweight='bold')\n",
    "    \n",
    "    # Activation comparison across conditions\n",
    "    ax = axes[0]\n",
    "    conditions = ['Gemma\\nText', 'PaliGemma\\nText', 'PaliGemma\\nMultimodal']\n",
    "    means = [analysis['gemma_mean'], analysis['pali_text_mean'], analysis['pali_mm_mean']]\n",
    "    colors = ['green', 'purple', 'blue']\n",
    "    bars = ax.bar(conditions, means, color=colors, alpha=0.7)\n",
    "    ax.set_ylabel('Mean Activation')\n",
    "    ax.set_title('Feature Activity by Condition')\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Histogram of multimodal activations\n",
    "    ax = axes[1]\n",
    "    ax.hist(feat_mm_acts.numpy(), bins=50, color='blue', alpha=0.7, edgecolor='black')\n",
    "    ax.set_xlabel('Activation Value')\n",
    "    ax.set_ylabel('Frequency')\n",
    "    ax.set_title('Multimodal Activation Distribution')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"figures/vision_feature_{feat_idx}.png\", dpi=150, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd6cbf2",
   "metadata": {},
   "source": [
    "## Summary Visualization: Feature Transfer Landscape\n",
    "\n",
    "A comprehensive view of how SAE features transfer from Gemma to PaliGemma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f290243",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# Cell 18: Summary Visualization\n",
    "# ==========================================\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "\n",
    "# Plot 1: 2D scatter of feature transfer (colored by category)\n",
    "ax = axes[0, 0]\n",
    "colors_map = {'stable': 'green', 'shifted': 'orange', 'new_vision': 'blue', 'new_text': 'purple'}\n",
    "for category, feat_indices in feature_classification.items():\n",
    "    if feat_indices:\n",
    "        g_acts = gemma_mean[feat_indices].numpy()\n",
    "        p_acts = pali_text_mean[feat_indices].numpy()\n",
    "        ax.scatter(g_acts, p_acts, label=category.replace('_', ' ').title(), \n",
    "                  color=colors_map[category], alpha=0.6, s=20)\n",
    "\n",
    "ax.plot([0, gemma_mean.max()], [0, gemma_mean.max()], 'k--', alpha=0.5, label='Perfect transfer')\n",
    "ax.set_xlabel(\"Gemma Mean Activation\")\n",
    "ax.set_ylabel(\"PaliGemma Text Mean Activation\")\n",
    "ax.set_title(\"Feature Transfer: Gemma  PaliGemma (Text)\")\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Vision specificity analysis\n",
    "ax = axes[0, 1]\n",
    "vision_boost = (pali_mm_mean / (pali_text_mean + 1e-6)).numpy()\n",
    "vision_boost = np.clip(vision_boost, 0, 10)  # Clip for visualization\n",
    "\n",
    "for category, feat_indices in feature_classification.items():\n",
    "    if feat_indices:\n",
    "        boosts = vision_boost[feat_indices]\n",
    "        ax.hist(boosts, bins=30, alpha=0.5, label=category.replace('_', ' ').title(),\n",
    "               color=colors_map[category])\n",
    "\n",
    "ax.axvline(x=1.0, color='red', linestyle='--', label='No vision boost')\n",
    "ax.set_xlabel(\"Vision Boost (Multimodal / Text)\")\n",
    "ax.set_ylabel(\"Count\")\n",
    "ax.set_title(\"Vision Specificity by Feature Category\")\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Feature reconstruction error comparison\n",
    "ax = axes[1, 0]\n",
    "# Compute reconstruction errors for each model\n",
    "with torch.no_grad():\n",
    "    gemma_recon, _ = sae(gemma_flat.to(DEVICE))\n",
    "    pali_text_recon, _ = sae(pali_text_flat.to(DEVICE))\n",
    "    pali_mm_recon, _ = sae(pali_mm_flat.to(DEVICE))\n",
    "    \n",
    "    gemma_error = F.mse_loss(gemma_recon, gemma_flat.to(DEVICE), reduction='none').mean(dim=1).cpu()\n",
    "    pali_text_error = F.mse_loss(pali_text_recon, pali_text_flat.to(DEVICE), reduction='none').mean(dim=1).cpu()\n",
    "    pali_mm_error = F.mse_loss(pali_mm_recon, pali_mm_flat.to(DEVICE), reduction='none').mean(dim=1).cpu()\n",
    "\n",
    "data = [gemma_error.numpy(), pali_text_error.numpy(), pali_mm_error.numpy()]\n",
    "labels = ['Gemma\\nText', 'PaliGemma\\nText', 'PaliGemma\\nMultimodal']\n",
    "bp = ax.boxplot(data, labels=labels, patch_artist=True)\n",
    "colors_box = ['green', 'purple', 'blue']\n",
    "for patch, color in zip(bp['boxes'], colors_box):\n",
    "    patch.set_facecolor(color)\n",
    "    patch.set_alpha(0.6)\n",
    "ax.set_ylabel(\"Reconstruction Error (MSE)\")\n",
    "ax.set_title(\"SAE Reconstruction Error by Condition\")\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Plot 4: Summary statistics table\n",
    "ax = axes[1, 1]\n",
    "ax.axis('off')\n",
    "\n",
    "# Create summary table\n",
    "summary_data = [\n",
    "    [\"Category\", \"Count\", \"Avg Gemma Act\", \"Avg PaliGemma Act\", \"Transfer Ratio\"],\n",
    "]\n",
    "\n",
    "for cat, indices in feature_classification.items():\n",
    "    if indices:\n",
    "        g_avg = gemma_mean[indices].mean().item()\n",
    "        p_avg = pali_text_mean[indices].mean().item()\n",
    "        ratio = p_avg / (g_avg + 1e-6)\n",
    "        summary_data.append([\n",
    "            cat.replace('_', ' ').title(),\n",
    "            str(len(indices)),\n",
    "            f\"{g_avg:.4f}\",\n",
    "            f\"{p_avg:.4f}\",\n",
    "            f\"{ratio:.2f}x\"\n",
    "        ])\n",
    "\n",
    "table = ax.table(\n",
    "    cellText=summary_data,\n",
    "    loc='center',\n",
    "    cellLoc='center',\n",
    "    colWidths=[0.25, 0.15, 0.2, 0.2, 0.2]\n",
    ")\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(10)\n",
    "table.scale(1.2, 1.5)\n",
    "\n",
    "# Make header row bold\n",
    "for i in range(5):\n",
    "    table[(0, i)].set_text_props(fontweight='bold')\n",
    "    table[(0, i)].set_facecolor('#4472C4')\n",
    "    table[(0, i)].set_text_props(color='white')\n",
    "\n",
    "ax.set_title(\"Feature Category Summary\", fontsize=12, fontweight='bold', pad=20)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"figures/sae_feature_transfer_summary.png\", dpi=200, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nSummary saved to figures/sae_feature_transfer_summary.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ffd3cb1",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "This experiment analyzed how SAE features transfer from Gemma-base to PaliGemma during multimodal adaptation:\n",
    "\n",
    "### Key Findings:\n",
    "\n",
    "1. **Stable Features**: Many features maintain similar activation patterns, suggesting core linguistic representations are preserved during multimodal training.\n",
    "\n",
    "2. **Shifted Features**: Some features are active in both models but with different patterns - these may represent concepts that gain visual grounding or change meaning in the multimodal context.\n",
    "\n",
    "3. **New Vision-Specific Features**: Features that activate primarily on image inputs in PaliGemma suggest the model develops specialized representations for visual information.\n",
    "\n",
    "### Implications for SAE Transfer:\n",
    "\n",
    "- Reconstruction error alone is indeed too coarse - feature-level analysis reveals a more nuanced picture of how representations change.\n",
    "- The SAE trained on Gemma can partially decode PaliGemma representations, but vision-specific information requires different features.\n",
    "- This analysis framework can guide decisions about when to retrain SAEs vs. adapt existing ones for multimodal models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6a649d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# Cell 19: Export Results Summary\n",
    "# ==========================================\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Create a DataFrame with detailed feature analysis\n",
    "results = []\n",
    "for category, indices in feature_classification.items():\n",
    "    for idx in indices[:50]:  # Limit to top 50 per category for export\n",
    "        results.append({\n",
    "            'feature_idx': idx,\n",
    "            'category': category,\n",
    "            'gemma_mean_activation': gemma_mean[idx].item(),\n",
    "            'gemma_max_activation': gemma_max[idx].item(),\n",
    "            'paligemma_text_mean': pali_text_mean[idx].item(),\n",
    "            'paligemma_text_max': pali_text_max[idx].item(),\n",
    "            'paligemma_mm_mean': pali_mm_mean[idx].item(),\n",
    "            'paligemma_mm_max': pali_mm_max[idx].item(),\n",
    "            'transfer_ratio': pali_text_mean[idx].item() / (gemma_mean[idx].item() + 1e-6),\n",
    "            'vision_boost': pali_mm_mean[idx].item() / (pali_text_mean[idx].item() + 1e-6),\n",
    "        })\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "df.to_csv('sae_feature_analysis_results.csv', index=False)\n",
    "\n",
    "print(f\"Exported {len(results)} feature analyses to sae_feature_analysis_results.csv\")\n",
    "print(\"\\nSample results:\")\n",
    "display(df.head(10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
