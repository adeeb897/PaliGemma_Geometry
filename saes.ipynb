{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3c64404",
   "metadata": {},
   "source": [
    "# Feature-level SAE Transfer & Semantics Analysis\n",
    "\n",
    "## Objective\n",
    "Analyze how individual SAE (Sparse Autoencoder) features change under multimodal adaptation from Gemma-base to PaliGemma.\n",
    "\n",
    "This experiment:\n",
    "1. Loads pre-trained SAE weights from GemmaScope (Google DeepMind's open SAEs for Gemma)\n",
    "2. Extracts hidden layer activations from both Gemma-base and PaliGemma\n",
    "3. Identifies interpretable features and categorizes them as:\n",
    "   - **Stable features**: Activate similarly across both models\n",
    "   - **Shifted features**: Exist in both but with different activation patterns\n",
    "   - **New features**: Only strongly active in PaliGemma (vision-specific)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8739e50",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'seaborn'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msns\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     14\u001b[39m     AutoModelForCausalLM,\n\u001b[32m     15\u001b[39m     AutoModelForImageTextToText,\n\u001b[32m     16\u001b[39m     AutoTokenizer,\n\u001b[32m     17\u001b[39m     AutoProcessor\n\u001b[32m     18\u001b[39m )\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mPIL\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Image\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'seaborn'"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# Cell 1: Install Dependencies and Imports\n",
    "# ==========================================\n",
    "# Uncomment below if needed:\n",
    "# %pip install sae-lens transformers torch matplotlib seaborn pandas tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoModelForImageTextToText,\n",
    "    AutoTokenizer,\n",
    "    AutoProcessor\n",
    ")\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import os\n",
    "import json\n",
    "from collections import defaultdict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e9364d",
   "metadata": {},
   "source": [
    "## Configuration and Model Setup\n",
    "\n",
    "We'll use:\n",
    "- **Gemma-2B**: The base language model\n",
    "- **PaliGemma-3B-pt-224**: The multimodal variant with vision capabilities\n",
    "\n",
    "For SAEs, we'll use GemmaScope SAEs which are publicly available from Google DeepMind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0efe98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# Cell 2: Configuration\n",
    "# ==========================================\n",
    "\n",
    "# Model configurations\n",
    "GEMMA_MODEL_NAME = \"google/gemma-2-2b\"  # Base Gemma model\n",
    "PALIGEMMA_MODEL_NAME = \"google/paligemma2-3b-pt-224\"  # Multimodal PaliGemma\n",
    "\n",
    "# Device configuration\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# SAE configuration - we'll extract from a middle layer\n",
    "# For Gemma-2B, good interpretable features are typically in layers 8-16\n",
    "TARGET_LAYER = 10  # Target hidden layer for analysis\n",
    "SAE_HIDDEN_DIM = 16384  # Typical SAE expansion factor of 8x for 2048-dim model\n",
    "\n",
    "# Analysis configuration\n",
    "NUM_FEATURES_TO_ANALYZE = 100  # Number of top features to analyze\n",
    "TOP_K_ACTIVATIONS = 20  # Top activations per feature to examine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa00d43",
   "metadata": {},
   "source": [
    "## Sparse Autoencoder Implementation\n",
    "\n",
    "We implement a simple SAE class that can:\n",
    "1. Train on model activations\n",
    "2. Encode activations to sparse feature representations  \n",
    "3. Decode back to original space\n",
    "\n",
    "The SAE uses a ReLU activation for sparsity and includes L1 regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e441ec76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# Cell 3: Sparse Autoencoder Implementation\n",
    "# ==========================================\n",
    "\n",
    "class SparseAutoencoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Sparse Autoencoder for learning interpretable features from model activations.\n",
    "    \n",
    "    Architecture:\n",
    "    - Encoder: Linear(d_model -> d_sae) + ReLU for sparsity\n",
    "    - Decoder: Linear(d_sae -> d_model) with tied weights optional\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        d_model: int, \n",
    "        d_sae: int, \n",
    "        dtype: torch.dtype = torch.float32,\n",
    "        device: torch.device = None\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_sae = d_sae\n",
    "        self.device = device or torch.device(\"cpu\")\n",
    "        \n",
    "        # Encoder: projects activations to sparse feature space\n",
    "        self.W_enc = nn.Parameter(torch.empty(d_model, d_sae, dtype=dtype, device=device))\n",
    "        self.b_enc = nn.Parameter(torch.zeros(d_sae, dtype=dtype, device=device))\n",
    "        \n",
    "        # Decoder: reconstructs original activations\n",
    "        self.W_dec = nn.Parameter(torch.empty(d_sae, d_model, dtype=dtype, device=device))\n",
    "        self.b_dec = nn.Parameter(torch.zeros(d_model, dtype=dtype, device=device))\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._init_weights()\n",
    "        \n",
    "    def _init_weights(self):\n",
    "        \"\"\"Initialize with Kaiming normal for encoder, transpose for decoder\"\"\"\n",
    "        nn.init.kaiming_normal_(self.W_enc, nonlinearity='relu')\n",
    "        # Initialize decoder as transpose of encoder (tied initialization)\n",
    "        self.W_dec.data = self.W_enc.data.T.clone()\n",
    "        \n",
    "    def encode(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Encode activations to sparse feature space.\n",
    "        \n",
    "        Args:\n",
    "            x: Activation tensor of shape [..., d_model]\n",
    "        Returns:\n",
    "            Sparse feature tensor of shape [..., d_sae]\n",
    "        \"\"\"\n",
    "        # Pre-encoder bias subtraction (center activations)\n",
    "        x_centered = x - self.b_dec\n",
    "        # Linear projection + ReLU for sparsity\n",
    "        return F.relu(x_centered @ self.W_enc + self.b_enc)\n",
    "    \n",
    "    def decode(self, features: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Decode sparse features back to activation space.\n",
    "        \n",
    "        Args:\n",
    "            features: Sparse feature tensor of shape [..., d_sae]\n",
    "        Returns:\n",
    "            Reconstructed activation tensor of shape [..., d_model]\n",
    "        \"\"\"\n",
    "        return features @ self.W_dec + self.b_dec\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Full forward pass: encode then decode.\n",
    "        \n",
    "        Returns:\n",
    "            - reconstructed: Reconstructed activations\n",
    "            - features: Sparse feature activations\n",
    "        \"\"\"\n",
    "        features = self.encode(x)\n",
    "        reconstructed = self.decode(features)\n",
    "        return reconstructed, features\n",
    "    \n",
    "    def get_feature_activations(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Get just the feature activations for analysis.\"\"\"\n",
    "        return self.encode(x)\n",
    "\n",
    "print(\"SparseAutoencoder class defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244b261c",
   "metadata": {},
   "source": [
    "## Load GemmaScope SAE Weights\n",
    "\n",
    "GemmaScope provides pre-trained SAEs for Gemma models. We'll download and load these weights.\n",
    "If unavailable, we'll train a simple SAE on collected activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6a814b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# Cell 4: Load GemmaScope SAE using sae-lens\n",
    "# ==========================================\n",
    "\n",
    "from sae_lens import SAE  # pip install sae-lens\n",
    "\n",
    "def load_gemmascope_sae(layer_idx: int = 10, width: str = \"16k\", device: torch.device = DEVICE):\n",
    "    \"\"\"\n",
    "    Load GemmaScope SAE weights using sae-lens library.\n",
    "    \n",
    "    GemmaScope SAEs are available at: gemma-scope-2b-pt-res-canonical\n",
    "    Format: layer_{layer_idx}/width_{width}/canonical\n",
    "    \"\"\"\n",
    "    try:\n",
    "        sae_id = f\"layer_{layer_idx}/width_{width}/canonical\"\n",
    "        print(f\"Loading GemmaScope SAE: {sae_id}\")\n",
    "        \n",
    "        sae, cfg_dict, sparsity = SAE.from_pretrained(\n",
    "            release=\"gemma-scope-2b-pt-res-canonical\",\n",
    "            sae_id=sae_id,\n",
    "            device=str(device),\n",
    "        )\n",
    "        \n",
    "        print(f\"  Loaded GemmaScope SAE for layer {layer_idx}\")\n",
    "        print(f\"  d_model (d_in): {sae.cfg.d_in}, d_sae: {sae.cfg.d_sae}\")\n",
    "        print(f\"  Sparsity (L0): {sparsity:.2f}\" if sparsity else \"  Sparsity: N/A\")\n",
    "        \n",
    "        return sae\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Could not load GemmaScope SAE: {e}\")\n",
    "        print(\"Falling back to training SAE from scratch...\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def train_simple_sae(\n",
    "    activations: torch.Tensor,\n",
    "    d_model: int,\n",
    "    d_sae: int,\n",
    "    num_epochs: int = 50,\n",
    "    batch_size: int = 256,\n",
    "    lr: float = 1e-3,\n",
    "    l1_coeff: float = 5e-4,\n",
    "    device: torch.device = DEVICE\n",
    "):\n",
    "    \"\"\"\n",
    "    Train a simple SAE on provided activations (fallback if GemmaScope unavailable).\n",
    "    \n",
    "    Args:\n",
    "        activations: Tensor of shape [N, d_model]\n",
    "        d_model: Model hidden dimension\n",
    "        d_sae: SAE hidden dimension (typically 8x d_model)\n",
    "        num_epochs: Training epochs\n",
    "        batch_size: Batch size\n",
    "        lr: Learning rate\n",
    "        l1_coeff: L1 sparsity regularization coefficient\n",
    "    \"\"\"\n",
    "    sae = SparseAutoencoder(d_model, d_sae, device=device).to(device)\n",
    "    optimizer = torch.optim.Adam(sae.parameters(), lr=lr)\n",
    "    \n",
    "    # Prepare data\n",
    "    activations = activations.to(device).float()\n",
    "    dataset = torch.utils.data.TensorDataset(activations)\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # Enable gradients for training\n",
    "    for param in sae.parameters():\n",
    "        param.requires_grad = True\n",
    "    \n",
    "    losses = []\n",
    "    with torch.enable_grad():\n",
    "        for epoch in tqdm(range(num_epochs), desc=\"Training SAE\"):\n",
    "            epoch_loss = 0\n",
    "            for batch in dataloader:\n",
    "                x = batch[0]\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # Forward pass\n",
    "                reconstructed, features = sae(x)\n",
    "                \n",
    "                # Reconstruction loss (MSE)\n",
    "                recon_loss = F.mse_loss(reconstructed, x)\n",
    "                \n",
    "                # Sparsity loss (L1 on feature activations)\n",
    "                l1_loss = l1_coeff * features.abs().mean()\n",
    "                \n",
    "                # Total loss\n",
    "                loss = recon_loss + l1_loss\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                # Normalize decoder weights (important for SAE stability)\n",
    "                with torch.no_grad():\n",
    "                    sae.W_dec.data = F.normalize(sae.W_dec.data, dim=1)\n",
    "                \n",
    "                epoch_loss += loss.item()\n",
    "            \n",
    "            losses.append(epoch_loss / len(dataloader))\n",
    "    \n",
    "    # Disable gradients after training\n",
    "    for param in sae.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    print(f\"Final loss: {losses[-1]:.4f}\")\n",
    "    return sae, losses\n",
    "\n",
    "\n",
    "print(\"SAE loading functions defined (using sae-lens)!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc963a37",
   "metadata": {},
   "source": [
    "## Load Models: Gemma-base and PaliGemma\n",
    "\n",
    "We load both models to extract and compare activations at the same layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4475f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# Cell 5: Load Models\n",
    "# ==========================================\n",
    "\n",
    "# Load Gemma-2B base model\n",
    "print(\"Loading Gemma-2B base model...\")\n",
    "gemma_model = AutoModelForCausalLM.from_pretrained(\n",
    "    GEMMA_MODEL_NAME,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=DEVICE,\n",
    ")\n",
    "gemma_tokenizer = AutoTokenizer.from_pretrained(GEMMA_MODEL_NAME)\n",
    "print(f\"  Loaded {GEMMA_MODEL_NAME}\")\n",
    "print(f\"  Number of layers: {gemma_model.config.num_hidden_layers}\")\n",
    "print(f\"  Hidden size: {gemma_model.config.hidden_size}\")\n",
    "\n",
    "# Load PaliGemma multimodal model\n",
    "print(\"\\nLoading PaliGemma multimodal model...\")\n",
    "paligemma_model = AutoModelForImageTextToText.from_pretrained(\n",
    "    PALIGEMMA_MODEL_NAME,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=DEVICE,\n",
    ")\n",
    "paligemma_processor = AutoProcessor.from_pretrained(PALIGEMMA_MODEL_NAME)\n",
    "paligemma_tokenizer = AutoTokenizer.from_pretrained(PALIGEMMA_MODEL_NAME)\n",
    "print(f\"  Loaded {PALIGEMMA_MODEL_NAME}\")\n",
    "print(f\"  Number of layers: {paligemma_model.language_model.config.num_hidden_layers}\")\n",
    "print(f\"  Hidden size: {paligemma_model.language_model.config.hidden_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87fbb362",
   "metadata": {},
   "source": [
    "## Activation Extraction Hooks\n",
    "\n",
    "We use forward hooks to capture hidden state activations at a specific layer.\n",
    "This allows us to extract activations without modifying the model code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd65800c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# Cell 6: Activation Extraction Class\n",
    "# ==========================================\n",
    "\n",
    "class ActivationExtractor:\n",
    "    \"\"\"\n",
    "    Extracts hidden layer activations from transformer models using forward hooks.\n",
    "    Works with both standard LLMs and multimodal models.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, target_layer: int, model_type: str = \"gemma\"):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            model: The transformer model\n",
    "            target_layer: Which layer to extract activations from\n",
    "            model_type: One of:\n",
    "                - \"gemma\": Base Gemma model (AutoModelForCausalLM)\n",
    "                - \"paligemma\": Full PaliGemma model (AutoModelForImageTextToText)\n",
    "                - \"paligemma_lm\": PaliGemma's language model directly (Gemma2Model)\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.target_layer = target_layer\n",
    "        self.model_type = model_type\n",
    "        self.activations = []\n",
    "        self.hook_handle = None\n",
    "        \n",
    "    def _get_target_module(self):\n",
    "        \"\"\"Get the module to attach the hook to.\"\"\"\n",
    "        if self.model_type == \"gemma\":\n",
    "            # AutoModelForCausalLM: model.model.layers[i]\n",
    "            return self.model.model.layers[self.target_layer]\n",
    "        elif self.model_type == \"paligemma\":\n",
    "            # AutoModelForImageTextToText: language_model is Gemma2Model with layers directly\n",
    "            return self.model.language_model.layers[self.target_layer]\n",
    "        elif self.model_type == \"paligemma_lm\":\n",
    "            # Direct Gemma2Model: layers are directly accessible\n",
    "            return self.model.layers[self.target_layer]\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown model type: {self.model_type}\")\n",
    "    \n",
    "    def _hook_fn(self, module, input, output):\n",
    "        \"\"\"Hook function to capture activations.\"\"\"\n",
    "        # Output is typically (hidden_states, ...) or just hidden_states\n",
    "        if isinstance(output, tuple):\n",
    "            hidden_states = output[0]\n",
    "        else:\n",
    "            hidden_states = output\n",
    "        # Flatten batch and sequence dims to [num_tokens, d_model]\n",
    "        # This handles variable sequence lengths across prompts\n",
    "        flat_activations = hidden_states.detach().cpu().float().reshape(-1, hidden_states.shape[-1])\n",
    "        self.activations.append(flat_activations)\n",
    "    \n",
    "    def attach_hook(self):\n",
    "        \"\"\"Attach the forward hook.\"\"\"\n",
    "        target_module = self._get_target_module()\n",
    "        self.hook_handle = target_module.register_forward_hook(self._hook_fn)\n",
    "        \n",
    "    def remove_hook(self):\n",
    "        \"\"\"Remove the forward hook.\"\"\"\n",
    "        if self.hook_handle:\n",
    "            self.hook_handle.remove()\n",
    "            self.hook_handle = None\n",
    "            \n",
    "    def clear_activations(self):\n",
    "        \"\"\"Clear stored activations.\"\"\"\n",
    "        self.activations = []\n",
    "        \n",
    "    def get_activations(self) -> torch.Tensor:\n",
    "        \"\"\"Get all collected activations as a single tensor of shape [total_tokens, d_model].\"\"\"\n",
    "        if not self.activations:\n",
    "            return None\n",
    "        return torch.cat(self.activations, dim=0)\n",
    "\n",
    "\n",
    "def extract_text_activations(\n",
    "    text_prompts: List[str],\n",
    "    model,\n",
    "    tokenizer,\n",
    "    extractor: ActivationExtractor,\n",
    ") -> Tuple[torch.Tensor, List[str]]:\n",
    "    \"\"\"\n",
    "    Extract activations for text-only prompts from a language model.\n",
    "    \n",
    "    Returns:\n",
    "        activations: Tensor of shape [total_tokens, d_model]\n",
    "        tokens: List of token strings corresponding to each activation\n",
    "    \"\"\"\n",
    "    extractor.clear_activations()\n",
    "    extractor.attach_hook()\n",
    "    \n",
    "    all_tokens = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for prompt in tqdm(text_prompts, desc=\"Extracting text activations\"):\n",
    "            inputs = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "            model(**inputs)\n",
    "            \n",
    "            # Decode tokens for reference\n",
    "            token_ids = inputs.input_ids[0].tolist()\n",
    "            tokens = [tokenizer.decode([tid]) for tid in token_ids]\n",
    "            all_tokens.extend(tokens)\n",
    "    \n",
    "    extractor.remove_hook()\n",
    "    activations = extractor.get_activations()\n",
    "    \n",
    "    return activations, all_tokens\n",
    "\n",
    "\n",
    "def extract_image_text_activations(\n",
    "    image_text_pairs: List[Tuple[Image.Image, str]],\n",
    "    model,\n",
    "    processor,\n",
    "    extractor: ActivationExtractor,\n",
    ") -> Tuple[torch.Tensor, List[Dict]]:\n",
    "    \"\"\"\n",
    "    Extract activations for image+text prompts from PaliGemma.\n",
    "    \n",
    "    Returns:\n",
    "        activations: Tensor of shape [total_tokens, d_model]\n",
    "        token_info: List of dicts with token info (is_image, token_str, etc.)\n",
    "    \"\"\"\n",
    "    extractor.clear_activations()\n",
    "    extractor.attach_hook()\n",
    "    \n",
    "    all_token_info = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for image, text in tqdm(image_text_pairs, desc=\"Extracting multimodal activations\"):\n",
    "            inputs = processor(\n",
    "                text=f\"<image>{text}\" if text else \"<image>\",\n",
    "                images=image,\n",
    "                return_tensors=\"pt\"\n",
    "            ).to(DEVICE)\n",
    "            \n",
    "            model(**inputs)\n",
    "            \n",
    "            # Track which tokens are image vs text\n",
    "            # PaliGemma uses <image> token that gets expanded to image embeddings\n",
    "            # The first N tokens are image tokens (typically 256 for 224x224 images)\n",
    "            num_image_tokens = (inputs.input_ids == processor.tokenizer.convert_tokens_to_ids(\"<image>\")).sum().item()\n",
    "            \n",
    "            # For simplicity, mark first batch of tokens as image-related\n",
    "            seq_len = inputs.input_ids.shape[1]\n",
    "            for i in range(seq_len):\n",
    "                all_token_info.append({\n",
    "                    \"is_image\": i < 256,  # Approximate, adjust based on actual image token count\n",
    "                    \"token_id\": inputs.input_ids[0, i].item() if i < inputs.input_ids.shape[1] else -1,\n",
    "                })\n",
    "    \n",
    "    extractor.remove_hook()\n",
    "    activations = extractor.get_activations()\n",
    "    \n",
    "    return activations, all_token_info\n",
    "\n",
    "\n",
    "print(\"Activation extraction classes defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075264e8",
   "metadata": {},
   "source": [
    "## Test Prompts for Feature Analysis\n",
    "\n",
    "We'll use a diverse set of prompts covering different semantic categories to identify interpretable features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ba7dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# Cell 7: Define Test Prompts and Images\n",
    "# ==========================================\n",
    "\n",
    "# Diverse text prompts to probe different semantic features\n",
    "TEXT_PROMPTS = [\n",
    "    # Numbers and counting\n",
    "    \"The answer is 42.\",\n",
    "    \"There are 3 apples and 5 oranges.\",\n",
    "    \"1, 2, 3, 4, 5, 6, 7, 8, 9, 10\",\n",
    "    \n",
    "    # Colors\n",
    "    \"The red ball bounced across the green grass.\",\n",
    "    \"She wore a beautiful blue dress to the party.\",\n",
    "    \"The yellow sun set behind the purple mountains.\",\n",
    "    \n",
    "    # Animals\n",
    "    \"The dog chased the cat around the garden.\",\n",
    "    \"A lion hunts zebras on the African savanna.\",\n",
    "    \"The elephant's trunk can hold gallons of water.\",\n",
    "    \n",
    "    # Spatial relationships\n",
    "    \"The book is on top of the table.\",\n",
    "    \"The cat is sitting under the chair.\",\n",
    "    \"The ball rolled between the two posts.\",\n",
    "    \n",
    "    # Actions/verbs\n",
    "    \"She ran quickly down the stairs.\",\n",
    "    \"He carefully opened the ancient door.\",\n",
    "    \"The bird flew gracefully through the sky.\",\n",
    "    \n",
    "    # Objects\n",
    "    \"The computer screen displayed an error message.\",\n",
    "    \"The car drove down the highway at high speed.\",\n",
    "    \"The camera captured the beautiful sunset.\",\n",
    "    \n",
    "    # Abstract concepts\n",
    "    \"Love is patient, love is kind.\",\n",
    "    \"The theory of relativity changed physics forever.\",\n",
    "    \"Democracy requires active citizen participation.\",\n",
    "    \n",
    "    # Code-related\n",
    "    \"def hello_world(): print('Hello!')\",\n",
    "    \"for i in range(10): sum += i\",\n",
    "    \"if x > 0: return True else: return False\",\n",
    "]\n",
    "\n",
    "# Download SPair-71k dataset if not present\n",
    "def download_spair71k():\n",
    "    \"\"\"Download and extract SPair-71k dataset.\"\"\"\n",
    "    import subprocess\n",
    "    \n",
    "    data_dir = \"data/SPair-71k_data\"\n",
    "    jpeg_dir = os.path.join(data_dir, \"SPair-71k/JPEGImages\")\n",
    "    \n",
    "    if os.path.exists(jpeg_dir):\n",
    "        print(\"SPair-71k dataset already exists.\")\n",
    "        return\n",
    "    \n",
    "    print(\"Downloading SPair-71k dataset...\")\n",
    "    os.makedirs(data_dir, exist_ok=True)\n",
    "    \n",
    "    # Download the dataset\n",
    "    url = \"https://cvlab.postech.ac.kr/research/SPair-71k/data/SPair-71k.tar.gz\"\n",
    "    tar_path = os.path.join(data_dir, \"SPair-71k.tar.gz\")\n",
    "    \n",
    "    # Use wget or curl depending on availability\n",
    "    try:\n",
    "        subprocess.run([\"wget\", \"-q\", \"-O\", tar_path, url], check=True)\n",
    "    except (subprocess.CalledProcessError, FileNotFoundError):\n",
    "        subprocess.run([\"curl\", \"-L\", \"-o\", tar_path, url], check=True)\n",
    "    \n",
    "    print(\"Extracting dataset...\")\n",
    "    import tarfile\n",
    "    with tarfile.open(tar_path, \"r:gz\") as tar:\n",
    "        tar.extractall(path=data_dir)\n",
    "    \n",
    "    # Clean up tar file\n",
    "    os.remove(tar_path)\n",
    "    print(\"SPair-71k dataset downloaded and extracted successfully!\")\n",
    "\n",
    "# Sample images for multimodal testing from SPair-71k dataset\n",
    "def get_sample_images() -> List[Tuple[Image.Image, str]]:\n",
    "    \"\"\"Load sample images from SPair-71k dataset for multimodal testing.\"\"\"\n",
    "    # Ensure dataset is downloaded\n",
    "    download_spair71k()\n",
    "    \n",
    "    image_text_pairs = []\n",
    "    \n",
    "    data_dir = \"data/SPair-71k_data/SPair-71k/JPEGImages\"\n",
    "    categories = [\"dog\", \"cat\", \"car\", \"bird\", \"horse\"]\n",
    "    \n",
    "    if not os.path.exists(data_dir):\n",
    "        raise FileNotFoundError(f\"Dataset not found at {data_dir}. Download failed.\")\n",
    "    \n",
    "    for category in categories:\n",
    "        cat_dir = os.path.join(data_dir, category)\n",
    "        if not os.path.exists(cat_dir):\n",
    "            raise FileNotFoundError(f\"Category '{category}' not found at {cat_dir}\")\n",
    "        \n",
    "        images = os.listdir(cat_dir)[:3]  # Take first 3 images per category\n",
    "        if len(images) == 0:\n",
    "            raise ValueError(f\"No images found in {cat_dir}\")\n",
    "        \n",
    "        for img_name in images:\n",
    "            img_path = os.path.join(cat_dir, img_name)\n",
    "            img = Image.open(img_path).convert(\"RGB\")\n",
    "            img = img.resize((224, 224))\n",
    "            image_text_pairs.append((img, f\"A photo of a {category}.\"))\n",
    "    \n",
    "    print(f\"Loaded {len(image_text_pairs)} image-text pairs from SPair-71k\")\n",
    "    return image_text_pairs\n",
    "\n",
    "print(f\"Defined {len(TEXT_PROMPTS)} text prompts for analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef58c57",
   "metadata": {},
   "source": [
    "## Extract Activations and Train/Load SAE\n",
    "\n",
    "Now we extract activations from both models and prepare the SAE for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3794fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# Cell 8: Extract Activations from Both Models\n",
    "# ==========================================\n",
    "\n",
    "# Create extractors for both models\n",
    "gemma_extractor = ActivationExtractor(gemma_model, TARGET_LAYER, model_type=\"gemma\")\n",
    "# Use \"paligemma_lm\" for direct language model access (text-only)\n",
    "paligemma_text_extractor = ActivationExtractor(\n",
    "    paligemma_model.language_model, TARGET_LAYER, model_type=\"paligemma_lm\"\n",
    ")\n",
    "# Use \"paligemma\" for full model with images\n",
    "paligemma_mm_extractor = ActivationExtractor(paligemma_model, TARGET_LAYER, model_type=\"paligemma\")\n",
    "\n",
    "# Extract text activations from Gemma\n",
    "print(\"Extracting activations from Gemma-2B...\")\n",
    "gemma_text_acts, gemma_tokens = extract_text_activations(\n",
    "    TEXT_PROMPTS, gemma_model, gemma_tokenizer, gemma_extractor\n",
    ")\n",
    "print(f\"  Gemma text activations shape: {gemma_text_acts.shape}\")\n",
    "\n",
    "# Extract text activations from PaliGemma (text-only, for fair comparison)\n",
    "print(\"\\nExtracting text activations from PaliGemma...\")\n",
    "paligemma_text_tokens = []\n",
    "\n",
    "paligemma_text_extractor.clear_activations()\n",
    "paligemma_text_extractor.attach_hook()\n",
    "\n",
    "for prompt in tqdm(TEXT_PROMPTS, desc=\"Extracting PaliGemma text activations\"):\n",
    "    # For PaliGemma, we use the language model directly for text-only inputs\n",
    "    inputs = paligemma_tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        paligemma_model.language_model(**inputs)\n",
    "    \n",
    "    token_ids = inputs.input_ids[0].tolist()\n",
    "    tokens = [paligemma_tokenizer.decode([tid]) for tid in token_ids]\n",
    "    paligemma_text_tokens.extend(tokens)\n",
    "\n",
    "paligemma_text_extractor.remove_hook()\n",
    "paligemma_text_acts = paligemma_text_extractor.get_activations()\n",
    "print(f\"  PaliGemma text activations shape: {paligemma_text_acts.shape}\")\n",
    "\n",
    "# Extract multimodal activations from PaliGemma\n",
    "print(\"\\nExtracting multimodal (image+text) activations from PaliGemma...\")\n",
    "image_text_pairs = get_sample_images()\n",
    "paligemma_mm_acts, paligemma_mm_info = extract_image_text_activations(\n",
    "    image_text_pairs, paligemma_model, paligemma_processor, paligemma_mm_extractor\n",
    ")\n",
    "print(f\"  PaliGemma multimodal activations shape: {paligemma_mm_acts.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0edaf1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# Cell 9: Load or Train SAE\n",
    "# ==========================================\n",
    "\n",
    "# Try to load GemmaScope SAE first\n",
    "sae = load_gemmascope_sae(layer_idx=TARGET_LAYER, device=DEVICE)\n",
    "\n",
    "# If GemmaScope not available, train on Gemma activations\n",
    "if sae is None:\n",
    "    print(\"Training SAE on Gemma activations...\")\n",
    "    d_model = gemma_text_acts.shape[-1]\n",
    "    d_sae = d_model * 8  # 8x expansion factor\n",
    "    \n",
    "    # Reshape activations for training [batch*seq, d_model]\n",
    "    train_acts = gemma_text_acts.reshape(-1, d_model)\n",
    "    \n",
    "    sae, train_losses = train_simple_sae(\n",
    "        train_acts,\n",
    "        d_model=d_model,\n",
    "        d_sae=d_sae,\n",
    "        num_epochs=100,\n",
    "        device=DEVICE\n",
    "    )\n",
    "    \n",
    "    # Plot training loss\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(train_losses)\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"SAE Training Loss\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    # Store dimensions for compatibility\n",
    "    sae.d_in = d_model\n",
    "    sae.d_sae = d_sae\n",
    "\n",
    "# Print SAE info\n",
    "if hasattr(sae, 'cfg'):\n",
    "    # sae-lens SAE\n",
    "    print(f\"\\nSAE ready (sae-lens): d_in={sae.cfg.d_in}, d_sae={sae.cfg.d_sae}\")\n",
    "else:\n",
    "    # Custom SAE\n",
    "    print(f\"\\nSAE ready (custom): d_model={sae.d_model}, d_sae={sae.d_sae}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3fec0c",
   "metadata": {},
   "source": [
    "## Feature-level Analysis: Identify Stable, Shifted, and New Features\n",
    "\n",
    "We now apply the SAE to both Gemma and PaliGemma activations, then classify features:\n",
    "- **Stable**: High correlation in activation patterns between models\n",
    "- **Shifted**: Active in both but with significantly different patterns\n",
    "- **New (Vision-specific)**: Only strongly active in PaliGemma multimodal inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2aa288c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# Cell 10: Compute SAE Feature Activations for All Conditions\n",
    "# ==========================================\n",
    "\n",
    "# Get SAE feature activations for each condition\n",
    "print(\"Computing SAE feature activations...\")\n",
    "\n",
    "# Reshape activations to [num_tokens, d_model] and move to device\n",
    "gemma_flat = gemma_text_acts.reshape(-1, gemma_text_acts.shape[-1]).to(DEVICE).float()\n",
    "pali_text_flat = paligemma_text_acts.reshape(-1, paligemma_text_acts.shape[-1]).to(DEVICE).float()\n",
    "pali_mm_flat = paligemma_mm_acts.reshape(-1, paligemma_mm_acts.shape[-1]).to(DEVICE).float()\n",
    "\n",
    "# Get SAE features - sae-lens SAE uses encode() method\n",
    "with torch.no_grad():\n",
    "    # sae-lens SAE.encode() returns feature activations\n",
    "    gemma_features = sae.encode(gemma_flat).cpu()\n",
    "    pali_text_features = sae.encode(pali_text_flat).cpu()\n",
    "    pali_mm_features = sae.encode(pali_mm_flat).cpu()\n",
    "\n",
    "print(f\"  Gemma text features shape: {gemma_features.shape}\")\n",
    "print(f\"  PaliGemma text features shape: {pali_text_features.shape}\")\n",
    "print(f\"  PaliGemma multimodal features shape: {pali_mm_features.shape}\")\n",
    "\n",
    "# Compute mean activation per feature\n",
    "gemma_mean = gemma_features.mean(dim=0)\n",
    "pali_text_mean = pali_text_features.mean(dim=0)\n",
    "pali_mm_mean = pali_mm_features.mean(dim=0)\n",
    "\n",
    "# Compute max activation per feature (better for sparse features)\n",
    "gemma_max = gemma_features.max(dim=0).values\n",
    "pali_text_max = pali_text_features.max(dim=0).values\n",
    "pali_mm_max = pali_mm_features.max(dim=0).values\n",
    "\n",
    "print(f\"\\nFeature activation statistics:\")\n",
    "print(f\"  Gemma - mean: {gemma_mean.mean():.4f}, nonzero: {(gemma_mean > 0).sum()}\")\n",
    "print(f\"  PaliGemma text - mean: {pali_text_mean.mean():.4f}, nonzero: {(pali_text_mean > 0).sum()}\")\n",
    "print(f\"  PaliGemma multimodal - mean: {pali_mm_mean.mean():.4f}, nonzero: {(pali_mm_mean > 0).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43e1190",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# Cell 11: Classify Features as Stable, Shifted, or New\n",
    "# ==========================================\n",
    "\n",
    "def classify_features(\n",
    "    gemma_features: torch.Tensor,\n",
    "    pali_text_features: torch.Tensor,\n",
    "    pali_mm_features: torch.Tensor,\n",
    "    stability_threshold: float = 0.7,\n",
    "    shift_threshold: float = 0.3,\n",
    "    activation_threshold: float = 0.1,\n",
    ") -> Dict[str, List[int]]:\n",
    "    \"\"\"\n",
    "    Classify SAE features into stable, shifted, and new categories.\n",
    "    \n",
    "    Args:\n",
    "        gemma_features: [N1, d_sae] features from Gemma on text\n",
    "        pali_text_features: [N2, d_sae] features from PaliGemma on text\n",
    "        pali_mm_features: [N3, d_sae] features from PaliGemma on images\n",
    "        stability_threshold: Correlation threshold for \"stable\" features\n",
    "        shift_threshold: Lower bound for \"shifted\" features\n",
    "        activation_threshold: Minimum activation to consider feature \"active\"\n",
    "    \n",
    "    Returns:\n",
    "        Dict with feature indices for each category\n",
    "    \"\"\"\n",
    "    num_features = gemma_features.shape[1]\n",
    "    \n",
    "    # Compute per-feature statistics\n",
    "    gemma_active = (gemma_features > activation_threshold).float().mean(dim=0)\n",
    "    pali_text_active = (pali_text_features > activation_threshold).float().mean(dim=0)\n",
    "    pali_mm_active = (pali_mm_features > activation_threshold).float().mean(dim=0)\n",
    "    \n",
    "    # Compute mean activations\n",
    "    gemma_mean = gemma_features.mean(dim=0)\n",
    "    pali_text_mean = pali_text_features.mean(dim=0)\n",
    "    pali_mm_mean = pali_mm_features.mean(dim=0)\n",
    "    \n",
    "    # Find features active in Gemma (for correlation analysis)\n",
    "    gemma_active_features = (gemma_active > 0.01).nonzero().squeeze(-1)\n",
    "    \n",
    "    classified = {\n",
    "        \"stable\": [],\n",
    "        \"shifted\": [],\n",
    "        \"new_vision\": [],\n",
    "        \"new_text\": [],\n",
    "    }\n",
    "    \n",
    "    # For each feature, determine its category\n",
    "    for feat_idx in range(num_features):\n",
    "        g_act = gemma_active[feat_idx].item()\n",
    "        pt_act = pali_text_active[feat_idx].item()\n",
    "        pm_act = pali_mm_active[feat_idx].item()\n",
    "        \n",
    "        g_mean = gemma_mean[feat_idx].item()\n",
    "        pt_mean = pali_text_mean[feat_idx].item()\n",
    "        pm_mean = pali_mm_mean[feat_idx].item()\n",
    "        \n",
    "        # Check if active in any condition\n",
    "        is_active_gemma = g_act > 0.01\n",
    "        is_active_pali_text = pt_act > 0.01\n",
    "        is_active_pali_mm = pm_act > 0.01\n",
    "        \n",
    "        if not (is_active_gemma or is_active_pali_text or is_active_pali_mm):\n",
    "            continue  # Skip dead features\n",
    "        \n",
    "        # New vision-specific: only active in multimodal, not in text-only\n",
    "        if is_active_pali_mm and not is_active_gemma and pm_mean > g_mean * 3:\n",
    "            classified[\"new_vision\"].append(feat_idx)\n",
    "            continue\n",
    "        \n",
    "        # New text features: active in PaliGemma text but not Gemma\n",
    "        if is_active_pali_text and not is_active_gemma and pt_mean > g_mean * 3:\n",
    "            classified[\"new_text\"].append(feat_idx)\n",
    "            continue\n",
    "        \n",
    "        # For features active in both, check similarity\n",
    "        if is_active_gemma and is_active_pali_text:\n",
    "            # Compute ratio of mean activations as stability measure\n",
    "            ratio = min(g_mean, pt_mean) / max(g_mean, pt_mean) if max(g_mean, pt_mean) > 0 else 0\n",
    "            \n",
    "            if ratio > stability_threshold:\n",
    "                classified[\"stable\"].append(feat_idx)\n",
    "            elif ratio > shift_threshold:\n",
    "                classified[\"shifted\"].append(feat_idx)\n",
    "    \n",
    "    return classified\n",
    "\n",
    "\n",
    "# Classify all features\n",
    "feature_classification = classify_features(\n",
    "    gemma_features, pali_text_features, pali_mm_features\n",
    ")\n",
    "\n",
    "print(\"Feature Classification Results:\")\n",
    "print(f\"  Stable features: {len(feature_classification['stable'])}\")\n",
    "print(f\"  Shifted features: {len(feature_classification['shifted'])}\")\n",
    "print(f\"  New vision-specific features: {len(feature_classification['new_vision'])}\")\n",
    "print(f\"  New text features (PaliGemma only): {len(feature_classification['new_text'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ebe06d",
   "metadata": {},
   "source": [
    "## Visualization: Feature Distribution Overview\n",
    "\n",
    "Let's visualize the overall distribution of feature activations across models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4f6fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# Cell 12: Feature Distribution Visualization\n",
    "# ==========================================\n",
    "\n",
    "# Ensure figures directory exists\n",
    "os.makedirs(\"figures\", exist_ok=True)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Plot 1: Scatter plot of mean activations (Gemma vs PaliGemma text)\n",
    "ax = axes[0, 0]\n",
    "ax.scatter(gemma_mean.numpy(), pali_text_mean.numpy(), alpha=0.3, s=5)\n",
    "ax.plot([0, gemma_mean.max()], [0, gemma_mean.max()], 'r--', label='y=x (perfect stability)')\n",
    "ax.set_xlabel(\"Gemma Mean Activation\")\n",
    "ax.set_ylabel(\"PaliGemma (Text) Mean Activation\")\n",
    "ax.set_title(\"Feature Activation Comparison: Text-only\")\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Scatter plot (Gemma vs PaliGemma multimodal)\n",
    "ax = axes[0, 1]\n",
    "ax.scatter(gemma_mean.numpy(), pali_mm_mean.numpy(), alpha=0.3, s=5)\n",
    "ax.plot([0, gemma_mean.max()], [0, gemma_mean.max()], 'r--', label='y=x')\n",
    "ax.set_xlabel(\"Gemma Mean Activation\")\n",
    "ax.set_ylabel(\"PaliGemma (Multimodal) Mean Activation\")\n",
    "ax.set_title(\"Feature Activation Comparison: Multimodal\")\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Distribution of feature activation ratios\n",
    "ax = axes[1, 0]\n",
    "# Avoid division by zero\n",
    "eps = 1e-6\n",
    "text_ratio = (pali_text_mean / (gemma_mean + eps)).numpy()\n",
    "mm_ratio = (pali_mm_mean / (gemma_mean + eps)).numpy()\n",
    "# Clip for visualization\n",
    "text_ratio = np.clip(text_ratio, 0, 10)\n",
    "mm_ratio = np.clip(mm_ratio, 0, 10)\n",
    "\n",
    "ax.hist(text_ratio, bins=50, alpha=0.5, label='Text ratio', density=True)\n",
    "ax.hist(mm_ratio, bins=50, alpha=0.5, label='Multimodal ratio', density=True)\n",
    "ax.axvline(x=1.0, color='r', linestyle='--', label='Ratio = 1')\n",
    "ax.set_xlabel(\"Activation Ratio (PaliGemma / Gemma)\")\n",
    "ax.set_ylabel(\"Density\")\n",
    "ax.set_title(\"Distribution of Feature Activation Ratios\")\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Feature category bar chart\n",
    "ax = axes[1, 1]\n",
    "categories = list(feature_classification.keys())\n",
    "counts = [len(feature_classification[cat]) for cat in categories]\n",
    "colors = ['green', 'orange', 'blue', 'purple']\n",
    "bars = ax.bar(categories, counts, color=colors, alpha=0.7)\n",
    "ax.set_xlabel(\"Feature Category\")\n",
    "ax.set_ylabel(\"Number of Features\")\n",
    "ax.set_title(\"Feature Classification Summary\")\n",
    "for bar, count in zip(bars, counts):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 5, \n",
    "            str(count), ha='center', va='bottom', fontweight='bold')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"figures/sae_feature_distribution.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0943719a",
   "metadata": {},
   "source": [
    "## Detailed Feature Examples: Stable, Shifted, and New Features\n",
    "\n",
    "Now we analyze 2-4 concrete examples from each category, showing:\n",
    "- What tokens/images maximally activate each feature\n",
    "- How activation patterns differ between Gemma and PaliGemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b119af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# Cell 13: Helper Functions for Feature Analysis\n",
    "# ==========================================\n",
    "\n",
    "# Get special tokens dynamically from tokenizers\n",
    "def get_special_token_set(*tokenizers):\n",
    "    \"\"\"Get the union of all special tokens from the provided tokenizers.\"\"\"\n",
    "    special_tokens = set()\n",
    "    for tokenizer in tokenizers:\n",
    "        # Get all special tokens list\n",
    "        if hasattr(tokenizer, 'all_special_tokens'):\n",
    "            special_tokens.update(tokenizer.all_special_tokens)\n",
    "        # Get special tokens from the map (values can be str or list)\n",
    "        if hasattr(tokenizer, 'special_tokens_map'):\n",
    "            for value in tokenizer.special_tokens_map.values():\n",
    "                if isinstance(value, str):\n",
    "                    special_tokens.add(value)\n",
    "                elif isinstance(value, list):\n",
    "                    special_tokens.update(value)\n",
    "        # Also get special token IDs and convert them\n",
    "        if hasattr(tokenizer, 'all_special_ids'):\n",
    "            for token_id in tokenizer.all_special_ids:\n",
    "                try:\n",
    "                    special_tokens.add(tokenizer.decode([token_id]))\n",
    "                except:\n",
    "                    pass\n",
    "    return special_tokens\n",
    "\n",
    "# Build special tokens set from our loaded tokenizers\n",
    "SPECIAL_TOKENS = get_special_token_set(gemma_tokenizer, paligemma_tokenizer)\n",
    "print(f\"Detected {len(SPECIAL_TOKENS)} special tokens: {SPECIAL_TOKENS}\")\n",
    "\n",
    "\n",
    "def is_meaningful_token(token: str, special_tokens: set = None) -> bool:\n",
    "    \"\"\"Check if a token is meaningful (not special token or whitespace-only).\n",
    "    \n",
    "    Args:\n",
    "        token: The token string to check\n",
    "        special_tokens: Set of special tokens to filter. Uses SPECIAL_TOKENS if None.\n",
    "    \"\"\"\n",
    "    if special_tokens is None:\n",
    "        special_tokens = SPECIAL_TOKENS\n",
    "    \n",
    "    token_stripped = token.strip()\n",
    "    \n",
    "    # Filter out empty/whitespace-only tokens\n",
    "    if not token_stripped:\n",
    "        return False\n",
    "    \n",
    "    # Filter out special tokens (check both stripped and original)\n",
    "    if token_stripped in special_tokens or token in special_tokens:\n",
    "        return False\n",
    "    \n",
    "    # Filter out tokens that are just whitespace variations\n",
    "    if token.replace(' ', '').replace('\\n', '').replace('\\t', '') == '':\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "\n",
    "def get_top_activating_examples(\n",
    "    feature_idx: int,\n",
    "    features: torch.Tensor,\n",
    "    tokens: List[str],\n",
    "    top_k: int = 10,\n",
    "    filter_special: bool = True\n",
    ") -> List[Tuple[str, float]]:\n",
    "    \"\"\"Get the top-k tokens that maximally activate a feature.\n",
    "    \n",
    "    Args:\n",
    "        feature_idx: Index of the feature to analyze\n",
    "        features: Tensor of shape [num_tokens, num_features]\n",
    "        tokens: List of token strings\n",
    "        top_k: Number of top activating tokens to return\n",
    "        filter_special: Whether to filter out special tokens and whitespace\n",
    "    \"\"\"\n",
    "    feature_acts = features[:, feature_idx]\n",
    "    \n",
    "    # Sort all indices by activation (descending)\n",
    "    sorted_indices = torch.argsort(feature_acts, descending=True)\n",
    "    \n",
    "    results = []\n",
    "    for idx in sorted_indices:\n",
    "        if idx >= len(tokens):\n",
    "            continue\n",
    "        token = tokens[idx.item()]\n",
    "        \n",
    "        # Filter special tokens if requested\n",
    "        if filter_special and not is_meaningful_token(token):\n",
    "            continue\n",
    "            \n",
    "        results.append((token, feature_acts[idx.item()].item()))\n",
    "        \n",
    "        if len(results) >= top_k:\n",
    "            break\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def analyze_single_feature(\n",
    "    feature_idx: int,\n",
    "    gemma_features: torch.Tensor,\n",
    "    pali_text_features: torch.Tensor,\n",
    "    pali_mm_features: torch.Tensor,\n",
    "    gemma_tokens: List[str],\n",
    "    pali_tokens: List[str],\n",
    "    category: str,\n",
    ") -> Dict:\n",
    "    \"\"\"Analyze a single feature across all conditions.\"\"\"\n",
    "    \n",
    "    # Get top activating tokens for each condition (filtering special tokens)\n",
    "    gemma_top = get_top_activating_examples(feature_idx, gemma_features, gemma_tokens, filter_special=True)\n",
    "    pali_text_top = get_top_activating_examples(feature_idx, pali_text_features, pali_tokens, filter_special=True)\n",
    "    \n",
    "    # Statistics\n",
    "    g_mean = gemma_features[:, feature_idx].mean().item()\n",
    "    g_max = gemma_features[:, feature_idx].max().item()\n",
    "    p_text_mean = pali_text_features[:, feature_idx].mean().item()\n",
    "    p_text_max = pali_text_features[:, feature_idx].max().item()\n",
    "    p_mm_mean = pali_mm_features[:, feature_idx].mean().item()\n",
    "    p_mm_max = pali_mm_features[:, feature_idx].max().item()\n",
    "    \n",
    "    return {\n",
    "        \"feature_idx\": feature_idx,\n",
    "        \"category\": category,\n",
    "        \"gemma_mean\": g_mean,\n",
    "        \"gemma_max\": g_max,\n",
    "        \"pali_text_mean\": p_text_mean,\n",
    "        \"pali_text_max\": p_text_max,\n",
    "        \"pali_mm_mean\": p_mm_mean,\n",
    "        \"pali_mm_max\": p_mm_max,\n",
    "        \"gemma_top_tokens\": gemma_top[:5],\n",
    "        \"pali_text_top_tokens\": pali_text_top[:5],\n",
    "    }\n",
    "\n",
    "\n",
    "def plot_feature_comparison(analysis: Dict, fig_num: int):\n",
    "    \"\"\"Create a detailed visualization for a single feature.\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "    fig.suptitle(f\"Feature #{analysis['feature_idx']} ({analysis['category'].upper()})\", \n",
    "                 fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # Plot 1: Activation statistics bar chart\n",
    "    ax = axes[0]\n",
    "    conditions = ['Gemma\\nText', 'PaliGemma\\nText', 'PaliGemma\\nMultimodal']\n",
    "    means = [analysis['gemma_mean'], analysis['pali_text_mean'], analysis['pali_mm_mean']]\n",
    "    maxs = [analysis['gemma_max'], analysis['pali_text_max'], analysis['pali_mm_max']]\n",
    "    \n",
    "    x = np.arange(len(conditions))\n",
    "    width = 0.35\n",
    "    bars1 = ax.bar(x - width/2, means, width, label='Mean', color='steelblue', alpha=0.8)\n",
    "    bars2 = ax.bar(x + width/2, maxs, width, label='Max', color='coral', alpha=0.8)\n",
    "    ax.set_ylabel('Activation')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(conditions)\n",
    "    ax.legend()\n",
    "    ax.set_title('Activation Statistics')\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Plot 2: Top activating tokens for Gemma\n",
    "    ax = axes[1]\n",
    "    gemma_tokens_display = [t[0][:20] for t in analysis['gemma_top_tokens']]\n",
    "    gemma_acts = [t[1] for t in analysis['gemma_top_tokens']]\n",
    "    if gemma_tokens_display:\n",
    "        y_pos = np.arange(len(gemma_tokens_display))\n",
    "        ax.barh(y_pos, gemma_acts, color='green', alpha=0.7)\n",
    "        ax.set_yticks(y_pos)\n",
    "        ax.set_yticklabels(gemma_tokens_display, fontsize=9)\n",
    "        ax.invert_yaxis()\n",
    "    ax.set_xlabel('Activation')\n",
    "    ax.set_title('Top Tokens (Gemma)')\n",
    "    ax.grid(True, alpha=0.3, axis='x')\n",
    "    \n",
    "    # Plot 3: Top activating tokens for PaliGemma\n",
    "    ax = axes[2]\n",
    "    pali_tokens_display = [t[0][:20] for t in analysis['pali_text_top_tokens']]\n",
    "    pali_acts = [t[1] for t in analysis['pali_text_top_tokens']]\n",
    "    if pali_tokens_display:\n",
    "        y_pos = np.arange(len(pali_tokens_display))\n",
    "        ax.barh(y_pos, pali_acts, color='purple', alpha=0.7)\n",
    "        ax.set_yticks(y_pos)\n",
    "        ax.set_yticklabels(pali_tokens_display, fontsize=9)\n",
    "        ax.invert_yaxis()\n",
    "    ax.set_xlabel('Activation')\n",
    "    ax.set_title('Top Tokens (PaliGemma)')\n",
    "    ax.grid(True, alpha=0.3, axis='x')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"figures/feature_{analysis['feature_idx']}_{analysis['category']}.png\", \n",
    "                dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "print(\"Feature analysis helper functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b72f227",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# Cell 14: Analyze Example Features from Each Category\n",
    "# ==========================================\n",
    "\n",
    "# Create figures directory if it doesn't exist\n",
    "os.makedirs(\"figures\", exist_ok=True)\n",
    "\n",
    "# Select top features from each category (by activity level)\n",
    "def select_top_features(feature_indices: List[int], features: torch.Tensor, n: int = 4) -> List[int]:\n",
    "    \"\"\"Select the n most active features from a list.\"\"\"\n",
    "    if not feature_indices:\n",
    "        return []\n",
    "    feature_means = features[:, feature_indices].mean(dim=0)\n",
    "    top_indices = torch.topk(feature_means, min(n, len(feature_indices))).indices\n",
    "    return [feature_indices[i] for i in top_indices.tolist()]\n",
    "\n",
    "# Get representative features from each category\n",
    "example_features = {}\n",
    "for category in ['stable', 'shifted', 'new_vision', 'new_text']:\n",
    "    if category == 'new_vision':\n",
    "        # Use multimodal features for selection\n",
    "        example_features[category] = select_top_features(\n",
    "            feature_classification[category], pali_mm_features, n=4\n",
    "        )\n",
    "    elif category == 'new_text':\n",
    "        example_features[category] = select_top_features(\n",
    "            feature_classification[category], pali_text_features, n=4\n",
    "        )\n",
    "    else:\n",
    "        example_features[category] = select_top_features(\n",
    "            feature_classification[category], gemma_features, n=4\n",
    "        )\n",
    "\n",
    "print(\"Selected example features for detailed analysis:\")\n",
    "for cat, feats in example_features.items():\n",
    "    print(f\"  {cat}: {feats}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61215d50",
   "metadata": {},
   "source": [
    "### Example 1: Stable Features\n",
    "\n",
    "These features maintain similar activation patterns across both Gemma and PaliGemma, suggesting they encode concepts that transfer well to the multimodal setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5bc01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# Cell 15: Analyze Stable Features\n",
    "# ==========================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"STABLE FEATURES - Similar activation patterns across models\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "stable_analyses = []\n",
    "for i, feat_idx in enumerate(example_features.get('stable', [])[:4]):\n",
    "    print(f\"\\n--- Stable Feature #{feat_idx} ---\")\n",
    "    analysis = analyze_single_feature(\n",
    "        feat_idx, gemma_features, pali_text_features, pali_mm_features,\n",
    "        gemma_tokens, paligemma_text_tokens, \"stable\"\n",
    "    )\n",
    "    stable_analyses.append(analysis)\n",
    "    \n",
    "    print(f\"  Gemma mean: {analysis['gemma_mean']:.4f}, max: {analysis['gemma_max']:.4f}\")\n",
    "    print(f\"  PaliGemma text mean: {analysis['pali_text_mean']:.4f}, max: {analysis['pali_text_max']:.4f}\")\n",
    "    print(f\"  PaliGemma multimodal mean: {analysis['pali_mm_mean']:.4f}\")\n",
    "    print(f\"  Top Gemma tokens: {[t[0] for t in analysis['gemma_top_tokens'][:3]]}\")\n",
    "    print(f\"  Top PaliGemma tokens: {[t[0] for t in analysis['pali_text_top_tokens'][:3]]}\")\n",
    "    \n",
    "    plot_feature_comparison(analysis, i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a991ce",
   "metadata": {},
   "source": [
    "### Example 2: Shifted Features\n",
    "\n",
    "These features exist in both models but with different activation patterns - suggesting semantic adaptation during multimodal training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b52202f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# Cell 16: Analyze Shifted Features\n",
    "# ==========================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"SHIFTED FEATURES - Active in both but with different patterns\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "shifted_analyses = []\n",
    "for i, feat_idx in enumerate(example_features.get('shifted', [])[:4]):\n",
    "    print(f\"\\n--- Shifted Feature #{feat_idx} ---\")\n",
    "    analysis = analyze_single_feature(\n",
    "        feat_idx, gemma_features, pali_text_features, pali_mm_features,\n",
    "        gemma_tokens, paligemma_text_tokens, \"shifted\"\n",
    "    )\n",
    "    shifted_analyses.append(analysis)\n",
    "    \n",
    "    # Calculate shift magnitude\n",
    "    shift_ratio = analysis['pali_text_mean'] / (analysis['gemma_mean'] + 1e-6)\n",
    "    print(f\"  Shift ratio (PaliGemma/Gemma): {shift_ratio:.2f}x\")\n",
    "    print(f\"  Gemma mean: {analysis['gemma_mean']:.4f}\")\n",
    "    print(f\"  PaliGemma text mean: {analysis['pali_text_mean']:.4f}\")\n",
    "    print(f\"  Top Gemma tokens: {[t[0] for t in analysis['gemma_top_tokens'][:3]]}\")\n",
    "    print(f\"  Top PaliGemma tokens: {[t[0] for t in analysis['pali_text_top_tokens'][:3]]}\")\n",
    "    \n",
    "    plot_feature_comparison(analysis, i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0edb1204",
   "metadata": {},
   "source": [
    "### Example 3: New Vision-Specific Features\n",
    "\n",
    "These features are primarily active in PaliGemma when processing image inputs, suggesting they've been specialized for visual information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3400744b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# Cell 17: Analyze New Vision-Specific Features\n",
    "# ==========================================\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"NEW VISION-SPECIFIC FEATURES - Active mainly on image inputs\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "vision_analyses = []\n",
    "for i, feat_idx in enumerate(example_features.get('new_vision', [])[:4]):\n",
    "    print(f\"\\n--- Vision-Specific Feature #{feat_idx} ---\")\n",
    "    analysis = analyze_single_feature(\n",
    "        feat_idx, gemma_features, pali_text_features, pali_mm_features,\n",
    "        gemma_tokens, paligemma_text_tokens, \"new_vision\"\n",
    "    )\n",
    "    vision_analyses.append(analysis)\n",
    "    \n",
    "    # Calculate vision specificity\n",
    "    vision_ratio = analysis['pali_mm_mean'] / (analysis['gemma_mean'] + 1e-6)\n",
    "    text_ratio = analysis['pali_text_mean'] / (analysis['gemma_mean'] + 1e-6)\n",
    "    print(f\"  Vision boost: {vision_ratio:.2f}x over Gemma\")\n",
    "    print(f\"  Text boost: {text_ratio:.2f}x over Gemma\")\n",
    "    print(f\"  Gemma mean: {analysis['gemma_mean']:.4f}\")\n",
    "    print(f\"  PaliGemma multimodal mean: {analysis['pali_mm_mean']:.4f}\")\n",
    "    \n",
    "    # For vision features, show activation distribution across image positions\n",
    "    feat_mm_acts = pali_mm_features[:, feat_idx]\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    fig.suptitle(f\"Vision Feature #{feat_idx}\", fontsize=12, fontweight='bold')\n",
    "    \n",
    "    # Activation comparison across conditions\n",
    "    ax = axes[0]\n",
    "    conditions = ['Gemma\\nText', 'PaliGemma\\nText', 'PaliGemma\\nMultimodal']\n",
    "    means = [analysis['gemma_mean'], analysis['pali_text_mean'], analysis['pali_mm_mean']]\n",
    "    colors = ['green', 'purple', 'blue']\n",
    "    bars = ax.bar(conditions, means, color=colors, alpha=0.7)\n",
    "    ax.set_ylabel('Mean Activation')\n",
    "    ax.set_title('Feature Activity by Condition')\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Histogram of multimodal activations\n",
    "    ax = axes[1]\n",
    "    ax.hist(feat_mm_acts.numpy(), bins=50, color='blue', alpha=0.7, edgecolor='black')\n",
    "    ax.set_xlabel('Activation Value')\n",
    "    ax.set_ylabel('Frequency')\n",
    "    ax.set_title('Multimodal Activation Distribution')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"figures/vision_feature_{feat_idx}.png\", dpi=150, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd6cbf2",
   "metadata": {},
   "source": [
    "## Summary Visualization: Feature Transfer Landscape\n",
    "\n",
    "A comprehensive view of how SAE features transfer from Gemma to PaliGemma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f290243",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# Cell 18: Summary Visualization\n",
    "# ==========================================\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "# Plot 1: 2D scatter of feature transfer (log scale for better visibility)\n",
    "ax = axes[0, 0]\n",
    "colors_map = {'stable': 'green', 'shifted': 'orange', 'new_vision': 'blue', 'new_text': 'purple'}\n",
    "for category, feat_indices in feature_classification.items():\n",
    "    if feat_indices:\n",
    "        # Add small epsilon to avoid log(0)\n",
    "        g_acts = gemma_mean[feat_indices].numpy() + 1e-4\n",
    "        p_acts = pali_text_mean[feat_indices].numpy() + 1e-4\n",
    "        ax.scatter(g_acts, p_acts, label=category.replace('_', ' ').title(), \n",
    "                  color=colors_map[category], alpha=0.6, s=20)\n",
    "\n",
    "# Diagonal line for perfect transfer\n",
    "max_val = max(gemma_mean.max().item(), pali_text_mean.max().item())\n",
    "ax.plot([1e-4, max_val], [1e-4, max_val], 'k--', alpha=0.5, label='Perfect transfer')\n",
    "ax.set_xscale('log')\n",
    "ax.set_yscale('log')\n",
    "ax.set_xlabel(\"Gemma Mean Activation (log scale)\")\n",
    "ax.set_ylabel(\"PaliGemma Text Mean Activation (log scale)\")\n",
    "ax.set_title(\"Feature Transfer: Gemma  PaliGemma (Text)\\n(Log Scale)\")\n",
    "ax.legend(fontsize=8)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Zoomed linear view (focus on main cluster)\n",
    "ax = axes[0, 1]\n",
    "for category, feat_indices in feature_classification.items():\n",
    "    if feat_indices:\n",
    "        g_acts = gemma_mean[feat_indices].numpy()\n",
    "        p_acts = pali_text_mean[feat_indices].numpy()\n",
    "        ax.scatter(g_acts, p_acts, label=category.replace('_', ' ').title(), \n",
    "                  color=colors_map[category], alpha=0.6, s=20)\n",
    "\n",
    "ax.plot([0, 15], [0, 15], 'k--', alpha=0.5, label='Perfect transfer')\n",
    "ax.set_xlim(0, 15)\n",
    "ax.set_ylim(0, 15)\n",
    "ax.set_xlabel(\"Gemma Mean Activation\")\n",
    "ax.set_ylabel(\"PaliGemma Text Mean Activation\")\n",
    "ax.set_title(\"Feature Transfer (Zoomed: 0-15 range)\\nMajority of features\")\n",
    "ax.legend(fontsize=8)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Add annotation about outliers\n",
    "n_outliers = ((gemma_mean > 15) | (pali_text_mean > 15)).sum().item()\n",
    "ax.annotate(f'{n_outliers} features with activation > 15\\n(shown in log plot)', \n",
    "            xy=(0.95, 0.05), xycoords='axes fraction', ha='right', fontsize=8,\n",
    "            bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "# Plot 3: Vision specificity analysis\n",
    "ax = axes[0, 2]\n",
    "vision_boost = (pali_mm_mean / (pali_text_mean + 1e-6)).numpy()\n",
    "vision_boost = np.clip(vision_boost, 0, 10)  # Clip for visualization\n",
    "\n",
    "for category, feat_indices in feature_classification.items():\n",
    "    if feat_indices:\n",
    "        boosts = vision_boost[feat_indices]\n",
    "        ax.hist(boosts, bins=30, alpha=0.5, label=category.replace('_', ' ').title(),\n",
    "               color=colors_map[category])\n",
    "\n",
    "ax.axvline(x=1.0, color='red', linestyle='--', label='No vision boost')\n",
    "ax.set_xlabel(\"Vision Boost (Multimodal / Text)\")\n",
    "ax.set_ylabel(\"Count\")\n",
    "ax.set_title(\"Vision Specificity by Feature Category\")\n",
    "ax.legend(fontsize=8)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Feature reconstruction error comparison\n",
    "ax = axes[1, 0]\n",
    "# Compute reconstruction errors for each model\n",
    "# sae-lens SAE uses decode(encode(x)) for reconstruction\n",
    "with torch.no_grad():\n",
    "    # Move to device for SAE encoding/decoding\n",
    "    gemma_flat_device = gemma_flat.to(DEVICE)\n",
    "    pali_text_flat_device = pali_text_flat.to(DEVICE)\n",
    "    pali_mm_flat_device = pali_mm_flat.to(DEVICE)\n",
    "    \n",
    "    gemma_recon = sae.decode(sae.encode(gemma_flat_device))\n",
    "    pali_text_recon = sae.decode(sae.encode(pali_text_flat_device))\n",
    "    pali_mm_recon = sae.decode(sae.encode(pali_mm_flat_device))\n",
    "    \n",
    "    gemma_error = F.mse_loss(gemma_recon, gemma_flat_device, reduction='none').mean(dim=1).cpu()\n",
    "    pali_text_error = F.mse_loss(pali_text_recon, pali_text_flat_device, reduction='none').mean(dim=1).cpu()\n",
    "    pali_mm_error = F.mse_loss(pali_mm_recon, pali_mm_flat_device, reduction='none').mean(dim=1).cpu()\n",
    "\n",
    "data = [gemma_error.numpy(), pali_text_error.numpy(), pali_mm_error.numpy()]\n",
    "labels = ['Gemma\\nText', 'PaliGemma\\nText', 'PaliGemma\\nMultimodal']\n",
    "bp = ax.boxplot(data, labels=labels, patch_artist=True)\n",
    "colors_box = ['green', 'purple', 'blue']\n",
    "for patch, color in zip(bp['boxes'], colors_box):\n",
    "    patch.set_facecolor(color)\n",
    "    patch.set_alpha(0.6)\n",
    "ax.set_ylabel(\"Reconstruction Error (MSE)\")\n",
    "ax.set_title(\"SAE Reconstruction Error by Condition\")\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Plot 5: Activation distribution comparison (new plot)\n",
    "ax = axes[1, 1]\n",
    "# Show distribution of mean activations for each condition\n",
    "bins = np.logspace(-4, 2.5, 50)  # Log-spaced bins\n",
    "ax.hist(gemma_mean.numpy() + 1e-4, bins=bins, alpha=0.5, label='Gemma', color='green')\n",
    "ax.hist(pali_text_mean.numpy() + 1e-4, bins=bins, alpha=0.5, label='PaliGemma Text', color='purple')\n",
    "ax.hist(pali_mm_mean.numpy() + 1e-4, bins=bins, alpha=0.5, label='PaliGemma MM', color='blue')\n",
    "ax.set_xscale('log')\n",
    "ax.set_xlabel(\"Mean Feature Activation (log scale)\")\n",
    "ax.set_ylabel(\"Number of Features\")\n",
    "ax.set_title(\"Distribution of Feature Activations\")\n",
    "ax.legend(fontsize=8)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 6: Summary statistics table\n",
    "ax = axes[1, 2]\n",
    "ax.axis('off')\n",
    "\n",
    "# Create summary table\n",
    "summary_data = [\n",
    "    [\"Category\", \"Count\", \"Avg Gemma Act\", \"Avg PaliGemma Act\", \"Transfer Ratio\"],\n",
    "]\n",
    "\n",
    "for cat, indices in feature_classification.items():\n",
    "    if indices:\n",
    "        g_avg = gemma_mean[indices].mean().item()\n",
    "        p_avg = pali_text_mean[indices].mean().item()\n",
    "        ratio = p_avg / (g_avg + 1e-6)\n",
    "        summary_data.append([\n",
    "            cat.replace('_', ' ').title(),\n",
    "            str(len(indices)),\n",
    "            f\"{g_avg:.4f}\",\n",
    "            f\"{p_avg:.4f}\",\n",
    "            f\"{ratio:.2f}x\"\n",
    "        ])\n",
    "\n",
    "table = ax.table(\n",
    "    cellText=summary_data,\n",
    "    loc='center',\n",
    "    cellLoc='center',\n",
    "    colWidths=[0.25, 0.15, 0.2, 0.2, 0.2]\n",
    ")\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(10)\n",
    "table.scale(1.2, 1.5)\n",
    "\n",
    "# Make header row bold\n",
    "for i in range(5):\n",
    "    table[(0, i)].set_text_props(fontweight='bold')\n",
    "    table[(0, i)].set_facecolor('#4472C4')\n",
    "    table[(0, i)].set_text_props(color='white')\n",
    "\n",
    "ax.set_title(\"Feature Category Summary\", fontsize=12, fontweight='bold', pad=20)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"figures/sae_feature_transfer_summary.png\", dpi=200, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nSummary saved to figures/sae_feature_transfer_summary.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ffd3cb1",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "This experiment analyzed how SAE features transfer from Gemma-base to PaliGemma during multimodal adaptation:\n",
    "\n",
    "### Key Findings:\n",
    "\n",
    "1. **Stable Features**: Many features maintain similar activation patterns, suggesting core linguistic representations are preserved during multimodal training.\n",
    "\n",
    "2. **Shifted Features**: Some features are active in both models but with different patterns - these may represent concepts that gain visual grounding or change meaning in the multimodal context.\n",
    "\n",
    "3. **New Vision-Specific Features**: Features that activate primarily on image inputs in PaliGemma suggest the model develops specialized representations for visual information.\n",
    "\n",
    "### Implications for SAE Transfer:\n",
    "\n",
    "- Reconstruction error alone is indeed too coarse - feature-level analysis reveals a more nuanced picture of how representations change.\n",
    "- The SAE trained on Gemma can partially decode PaliGemma representations, but vision-specific information requires different features.\n",
    "- This analysis framework can guide decisions about when to retrain SAEs vs. adapt existing ones for multimodal models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6a649d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# Cell 19: Export Results Summary\n",
    "# ==========================================\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Create a DataFrame with detailed feature analysis\n",
    "results = []\n",
    "for category, indices in feature_classification.items():\n",
    "    for idx in indices[:50]:  # Limit to top 50 per category for export\n",
    "        results.append({\n",
    "            'feature_idx': idx,\n",
    "            'category': category,\n",
    "            'gemma_mean_activation': gemma_mean[idx].item(),\n",
    "            'gemma_max_activation': gemma_max[idx].item(),\n",
    "            'paligemma_text_mean': pali_text_mean[idx].item(),\n",
    "            'paligemma_text_max': pali_text_max[idx].item(),\n",
    "            'paligemma_mm_mean': pali_mm_mean[idx].item(),\n",
    "            'paligemma_mm_max': pali_mm_max[idx].item(),\n",
    "            'transfer_ratio': pali_text_mean[idx].item() / (gemma_mean[idx].item() + 1e-6),\n",
    "            'vision_boost': pali_mm_mean[idx].item() / (pali_text_mean[idx].item() + 1e-6),\n",
    "        })\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "df.to_csv('sae_feature_analysis_results.csv', index=False)\n",
    "\n",
    "print(f\"Exported {len(results)} feature analyses to sae_feature_analysis_results.csv\")\n",
    "print(\"\\nSample results:\")\n",
    "display(df.head(10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
